from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, RedirectResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
import json
import uuid
from datetime import datetime, timezone, timedelta
import os
from openai import AsyncOpenAI
import openai  # ì—ëŸ¬ ì²˜ë¦¬ìš©
from contextlib import asynccontextmanager
from dotenv import load_dotenv
import PyPDF2
import docx
from PIL import Image
import pytesseract
import io
import tempfile
import logging
import base64

# Google ì„œë¹„ìŠ¤ import
try:
    from google_services import auth_service, calendar_service, gmail_service
    from google_functions import GOOGLE_TOOLS, FUNCTION_MAP

    GOOGLE_SERVICES_AVAILABLE = True
    print("âœ… Google ì„œë¹„ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except ImportError as e:
    print(f"âš ï¸ Google ì„œë¹„ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    GOOGLE_SERVICES_AVAILABLE = False
    GOOGLE_TOOLS = []
    FUNCTION_MAP = {}

# Title Generator import
try:
    from title_generator import TitleGenerator
    TITLE_GENERATOR_AVAILABLE = True
    print("âœ… Title Generatorê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except ImportError as e:
    print(f"âš ï¸ Title Generator ë¡œë“œ ì‹¤íŒ¨: {e}")
    TITLE_GENERATOR_AVAILABLE = False

# ìƒˆë¡œìš´ AI Tools ì‹œìŠ¤í…œ import
try:
    from tools.manager import tool_manager

    TOOLS_SYSTEM_AVAILABLE = True
    print("âœ… AI Tools ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“Š ë“±ë¡ëœ ë„êµ¬ ìƒíƒœ: {tool_manager.get_status()}")
except ImportError as e:
    print(f"âš ï¸ AI Tools ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    TOOLS_SYSTEM_AVAILABLE = False
    tool_manager = None

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
)

# Title Generator ì´ˆê¸°í™”
title_generator = None
if TITLE_GENERATOR_AVAILABLE:
    title_generator = TitleGenerator(client)

# ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ì„¤ì •
AVAILABLE_MODELS = {
    "gpt-4o": {
        "name": "GPT-4o",
        "description": "OpenAIì˜ ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸",
        "provider": "openai",
        "supports_web_search": True,
        "supports_assistant": True,
        "max_tokens": 4000,
        "temperature": 0.7
    },
    "gpt-4": {
        "name": "GPT-4",
        "description": "OpenAIì˜ ê°•ë ¥í•œ ì–¸ì–´ ëª¨ë¸",
        "provider": "openai",
        "supports_web_search": True,
        "supports_assistant": True,
        "max_tokens": 4000,
        "temperature": 0.7
    },
    "gpt-3.5-turbo": {
        "name": "GPT-3.5 Turbo",
        "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ OpenAI ëª¨ë¸",
        "provider": "openai",
        "supports_web_search": False,
        "supports_assistant": False,
        "max_tokens": 2000,
        "temperature": 0.7
    }
}

# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©)
sessions_db: Dict[str, Dict] = {}
messages_db: Dict[str, List[Dict]] = {}
assistants_db: Dict[str, str] = {}  # session_id -> assistant_id ë§¤í•‘
threads_db: Dict[str, str] = {}  # session_id -> thread_id ë§¤í•‘

# ğŸ“Š í† í° ê´€ë¦¬ ë° ìµœì í™”
token_usage_db: Dict[str, Dict] = {}  # session_id -> token usage stats
conversation_summaries: Dict[str, str] = {}  # session_id -> summary

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ì„¤ì •
MAX_CONVERSATION_TOKENS = 8000  # ëŒ€í™”ë‹¹ ìµœëŒ€ í† í°
SUMMARY_TRIGGER_TOKENS = 6000  # ìš”ì•½ íŠ¸ë¦¬ê±° í† í°
MAX_MESSAGES_PER_SESSION = 50  # ì„¸ì…˜ë‹¹ ìµœëŒ€ ë©”ì‹œì§€

# ğŸ—‚ï¸ ë²¡í„° ìŠ¤í† ì–´ ë° ì§€ì‹ ë² ì´ìŠ¤ ê´€ë¦¬
vector_stores_db: Dict[str, str] = {}  # session_id -> vector_store_id ë§¤í•‘
knowledge_base_id: str = None  # ì „ì—­ ì§€ì‹ ë² ì´ìŠ¤ ë²¡í„° ìŠ¤í† ì–´ ID


# ===========================
# ğŸ—‚ï¸ ë²¡í„° ìŠ¤í† ì–´ ê¸°ëŠ¥ êµ¬í˜„
# ===========================

async def create_or_get_vector_store(session_id: str = None, name: str = None) -> str:
    """ì„¸ì…˜ë³„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë°˜í™˜"""
    try:
        # ì„¸ì…˜ë³„ ë²¡í„° ìŠ¤í† ì–´ í™•ì¸
        if session_id and session_id in vector_stores_db:
            return vector_stores_db[session_id]

        # ìƒˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vector_store_name = name or f"Session Vector Store {session_id or 'Global'}"
        vector_store = await client.beta.vector_stores.create(
            name=vector_store_name,
            file_ids=[],  # ì´ˆê¸°ì—ëŠ” ë¹ˆ ìƒíƒœë¡œ ìƒì„±
            metadata={
                "session_id": session_id or "global",
                "created_at": datetime.now().isoformat(),
                "purpose": "knowledge_base"
            }
        )

        # ë²¡í„° ìŠ¤í† ì–´ ID ì €ì¥
        vector_store_id = vector_store.id
        if session_id:
            vector_stores_db[session_id] = vector_store_id
        else:
            global knowledge_base_id
            knowledge_base_id = vector_store_id

        logger.info(f"âœ… Vector store created: {vector_store_id} for session: {session_id}")
        return vector_store_id

    except Exception as e:
        logger.error(f"âŒ Vector store creation failed: {str(e)}")
        raise


async def add_file_to_vector_store(vector_store_id: str, file_id: str) -> bool:
    """ë²¡í„° ìŠ¤í† ì–´ì— íŒŒì¼ ì¶”ê°€"""
    try:
        # íŒŒì¼ì„ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        vector_store_file = await client.beta.vector_stores.files.create_and_poll(
            vector_store_id=vector_store_id,
            file_id=file_id
        )

        logger.info(f"âœ… File {file_id} added to vector store {vector_store_id}")
        return vector_store_file.status == "completed"

    except Exception as e:
        logger.error(f"âŒ Failed to add file to vector store: {str(e)}")
        return False


async def search_vector_store(vector_store_id: str, query: str, limit: int = 5) -> List[Dict]:
    """ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        search_results = await client.beta.vector_stores.search(
            vector_store_id=vector_store_id,
            query=query,
            limit=limit
        )

        # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in search_results.data:
            formatted_results.append({
                "content": result.content if hasattr(result, 'content') else "",
                "score": result.score if hasattr(result, 'score') else 0.0,
                "file_id": result.file_id if hasattr(result, 'file_id') else "",
                "metadata": result.metadata if hasattr(result, 'metadata') else {}
            })

        logger.info(f"âœ… Vector search completed: {len(formatted_results)} results")
        return formatted_results

    except Exception as e:
        logger.error(f"âŒ Vector store search failed: {str(e)}")
        return []


async def create_knowledge_base_embeddings(documents: List[str], session_id: str = None) -> str:
    """ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±"""
    try:
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        vector_store_id = await create_or_get_vector_store(session_id, "Knowledge Base")

        # ê° ë¬¸ì„œë¥¼ íŒŒì¼ë¡œ ì—…ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        uploaded_files = []
        for i, document in enumerate(documents):
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False)
            temp_file.write(document)
            temp_file.close()

            try:
                # OpenAI Files APIë¡œ ì—…ë¡œë“œ
                with open(temp_file.name, 'rb') as f:
                    file_object = await client.files.create(
                        file=f,
                        purpose="assistants"
                    )

                # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
                if await add_file_to_vector_store(vector_store_id, file_object.id):
                    uploaded_files.append(file_object.id)

            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                try:
                    os.unlink(temp_file.name)
                except:
                    pass

        logger.info(f"âœ… Knowledge base created with {len(uploaded_files)} documents")
        return vector_store_id

    except Exception as e:
        logger.error(f"âŒ Knowledge base creation failed: {str(e)}")
        raise


async def get_relevant_context(query: str, session_id: str = None) -> str:
    """ì¿¼ë¦¬ì— ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    try:
        # ì„¸ì…˜ë³„ ë²¡í„° ìŠ¤í† ì–´ í™•ì¸
        vector_store_id = None
        if session_id and session_id in vector_stores_db:
            vector_store_id = vector_stores_db[session_id]
        elif knowledge_base_id:
            vector_store_id = knowledge_base_id

        if not vector_store_id:
            logger.info("No vector store available for context search")
            return ""

        # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        search_results = await search_vector_store(vector_store_id, query, limit=3)

        if not search_results:
            return ""

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        context_parts = []
        for result in search_results:
            if result.get("content"):
                context_parts.append(f"ê´€ë ¨ ì •ë³´: {result['content']}")

        return "\n\n".join(context_parts)

    except Exception as e:
        logger.error(f"âŒ Context retrieval failed: {str(e)}")
        return ""


async def list_vector_stores() -> List[Dict]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„° ìŠ¤í† ì–´ ëª©ë¡ ì¡°íšŒ"""
    try:
        vector_stores = await client.beta.vector_stores.list(limit=20)

        stores_info = []
        for store in vector_stores.data:
            stores_info.append({
                "id": store.id,
                "name": store.name,
                "file_counts": store.file_counts,
                "created_at": store.created_at,
                "metadata": store.metadata if hasattr(store, 'metadata') else {}
            })

        return stores_info

    except Exception as e:
        logger.error(f"âŒ Failed to list vector stores: {str(e)}")
        return []


# ğŸ“Š í† í° ê´€ë¦¬ ë° ìµœì í™” í•¨ìˆ˜ë“¤
def estimate_tokens(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ì¶”ì • (1 í† í° â‰ˆ 4 characters)"""
    return max(1, len(text) // 4)


def calculate_conversation_tokens(messages: List[Dict]) -> int:
    """ëŒ€í™”ì˜ ì´ í† í° ìˆ˜ ê³„ì‚°"""
    total_tokens = 0
    for message in messages:
        content = message.get("content", "")
        total_tokens += estimate_tokens(content)
    return total_tokens


async def create_conversation_summary(session_id: str, messages: List[Dict]) -> str:
    """ëŒ€í™” ìš”ì•½ ìƒì„±"""
    try:
        print(f"ğŸ“ Creating conversation summary for session: {session_id}")

        # ìš”ì•½í•  ë©”ì‹œì§€ë“¤ ì¤€ë¹„ (ìµœê·¼ 20ê°œë§Œ)
        messages_to_summarize = messages[-20:] if len(messages) > 20 else messages

        conversation_text = ""
        for msg in messages_to_summarize:
            role = msg.get("role", "")
            content = msg.get("content", "")
            conversation_text += f"{role}: {content}\n"

        # OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±
        summary_response = await client.chat.completions.create(
            model="gpt-3.5-turbo",  # ìš”ì•½ì€ ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ìš”ì•½ í˜•ì‹:
- ì£¼ìš” ì£¼ì œì™€ ë…¼ì˜ ë‚´ìš©
- í•µì‹¬ ê²°ë¡ ì´ë‚˜ ê²°ì •ì‚¬í•­
- ì¤‘ìš”í•œ ë°ì´í„°ë‚˜ ì •ë³´
- ì‚¬ìš©ì ê´€ì‹¬ì‚¬ë‚˜ ìš”êµ¬ì‚¬í•­

í•œêµ­ì–´ë¡œ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."""
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{conversation_text}"
                }
            ],
            max_tokens=200,
            temperature=0.3
        )

        summary = summary_response.choices[0].message.content

        # ìš”ì•½ ì €ì¥
        conversation_summaries[session_id] = summary
        print(f"âœ… Summary created for session {session_id}")

        return summary

    except Exception as e:
        print(f"ğŸš¨ Failed to create summary: {e}")
        return "ëŒ€í™” ìš”ì•½ ìƒì„± ì‹¤íŒ¨"


async def optimize_conversation_for_tokens(session_id: str) -> List[Dict]:
    """í† í° ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ëŒ€í™” ìµœì í™”"""
    messages = messages_db.get(session_id, [])

    if not messages:
        return []

    current_tokens = calculate_conversation_tokens(messages)
    print(f"ğŸ“Š Current conversation tokens: {current_tokens}")

    # í† í° í•œê³„ ì´ˆê³¼ ì‹œ ì²˜ë¦¬
    if current_tokens > MAX_CONVERSATION_TOKENS or len(messages) > MAX_MESSAGES_PER_SESSION:
        print(f"ğŸš¨ Token limit exceeded, optimizing conversation...")

        # 1. ëŒ€í™” ìš”ì•½ ìƒì„± (ì•„ì§ ì—†ë‹¤ë©´)
        if session_id not in conversation_summaries:
            await create_conversation_summary(session_id, messages[:-10])  # ìµœê·¼ 10ê°œ ì œì™¸í•˜ê³  ìš”ì•½

        # 2. ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ìš”ì•½ + ìµœê·¼ ëŒ€í™”)
        recent_messages = messages[-15:]  # ìµœê·¼ 15ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€

        # 3. ìš”ì•½ì„ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì‚½ì…
        summary = conversation_summaries.get(session_id, "")
        if summary:
            summary_message = {
                "role": "system",
                "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary}\n\nìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§€ì†ì ì´ê³  ì¼ê´€ëœ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”."
            }
            optimized_messages = [summary_message] + recent_messages
        else:
            optimized_messages = recent_messages

        # 4. ìµœì í™”ëœ ë©”ì‹œì§€ë¡œ ì—…ë°ì´íŠ¸
        messages_db[session_id] = optimized_messages

        # 5. í† í° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
        new_tokens = calculate_conversation_tokens(optimized_messages)
        update_token_usage(session_id, current_tokens, new_tokens, optimized=True)

        print(f"âœ… Conversation optimized: {current_tokens} â†’ {new_tokens} tokens")
        return optimized_messages

    else:
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        update_token_usage(session_id, current_tokens, current_tokens)
        return messages


def update_token_usage(session_id: str, old_tokens: int, new_tokens: int, optimized: bool = False):
    """í† í° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸"""
    if session_id not in token_usage_db:
        token_usage_db[session_id] = {
            "total_tokens": 0,
            "messages_count": 0,
            "optimizations": 0,
            "last_updated": datetime.now().isoformat()
        }

    usage = token_usage_db[session_id]
    usage["total_tokens"] = new_tokens
    usage["messages_count"] = len(messages_db.get(session_id, []))
    usage["last_updated"] = datetime.now().isoformat()

    if optimized:
        usage["optimizations"] += 1
        usage["tokens_saved"] = usage.get("tokens_saved", 0) + (old_tokens - new_tokens)


async def get_optimized_conversation_messages(session_id: str, max_messages: int = 20) -> List[Dict]:
    """ìµœì í™”ëœ ëŒ€í™” ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°"""
    # ë¨¼ì € í† í° ìµœì í™” ìˆ˜í–‰
    optimized_messages = await optimize_conversation_for_tokens(session_id)

    # ìš”ì²­ëœ ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ë¡œ ì œí•œ
    if len(optimized_messages) > max_messages:
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆë‹¤ë©´ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ ì œí•œ
        system_messages = [msg for msg in optimized_messages if msg.get("role") == "system"]
        user_assistant_messages = [msg for msg in optimized_messages if msg.get("role") in ["user", "assistant"]]

        # ìµœê·¼ ë©”ì‹œì§€ë“¤ ì„ íƒ
        recent_messages = user_assistant_messages[-(max_messages - len(system_messages)):]
        return system_messages + recent_messages

    return optimized_messages


# Google ì„œë¹„ìŠ¤ ë„êµ¬ ì •ì˜
def get_google_tools():
    """Google ì„œë¹„ìŠ¤ í•¨ìˆ˜ë“¤ì„ OpenAI ë„êµ¬ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    return [
        {
            "type": "function",
            "function": {
                "name": "get_calendar_events",
                "description": "Google Calendarì—ì„œ ì¼ì •ì„ ì¡°íšŒí•©ë‹ˆë‹¤. ì˜¤ëŠ˜, ì´ë²ˆì£¼, ì´ë²ˆë‹¬ ë“±ì˜ ì¼ì •ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "time_period": {
                            "type": "string",
                            "enum": ["today", "tomorrow", "this_week", "next_week", "this_month", "next_month"],
                            "description": "ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„"
                        },
                        "max_results": {
                            "type": "integer",
                            "default": 10,
                            "description": "ìµœëŒ€ ì¡°íšŒí•  ì¼ì • ìˆ˜"
                        }
                    },
                    "required": ["time_period"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "create_calendar_event",
                "description": "Google Calendarì— ìƒˆë¡œìš´ ì¼ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "summary": {
                            "type": "string",
                            "description": "ì¼ì • ì œëª©"
                        },
                        "description": {
                            "type": "string",
                            "description": "ì¼ì • ì„¤ëª…"
                        },
                        "start_datetime": {
                            "type": "string",
                            "description": "ì‹œì‘ ì¼ì‹œ (ISO 8601 í˜•ì‹: 2023-12-25T10:00:00)"
                        },
                        "end_datetime": {
                            "type": "string",
                            "description": "ì¢…ë£Œ ì¼ì‹œ (ISO 8601 í˜•ì‹: 2023-12-25T11:00:00)"
                        },
                        "attendees": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ì°¸ì„ì ì´ë©”ì¼ ëª©ë¡"
                        }
                    },
                    "required": ["summary", "start_datetime", "end_datetime"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "find_free_time",
                "description": "ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆ ë¹ˆ ì‹œê°„ì„ ì°¾ìŠµë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "duration_minutes": {
                            "type": "integer",
                            "description": "í•„ìš”í•œ ì‹œê°„ (ë¶„ ë‹¨ìœ„)"
                        },
                        "date_range": {
                            "type": "string",
                            "enum": ["today", "tomorrow", "this_week", "next_week"],
                            "description": "ê²€ìƒ‰í•  ë‚ ì§œ ë²”ìœ„"
                        },
                        "working_hours_only": {
                            "type": "boolean",
                            "default": True,
                            "description": "ì—…ë¬´ ì‹œê°„(9-18ì‹œ)ë§Œ ê²€ìƒ‰í• ì§€ ì—¬ë¶€"
                        }
                    },
                    "required": ["duration_minutes", "date_range"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_emails",
                "description": "Gmailì—ì„œ ì´ë©”ì¼ì„ ì¡°íšŒí•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: 'subject:íšŒì˜', 'from:manager@company.com')"
                        },
                        "max_results": {
                            "type": "integer",
                            "default": 10,
                            "description": "ìµœëŒ€ ì¡°íšŒí•  ì´ë©”ì¼ ìˆ˜"
                        },
                        "time_period": {
                            "type": "string",
                            "enum": ["today", "this_week", "this_month", "all"],
                            "default": "this_week",
                            "description": "ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„"
                        }
                    }
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "send_email",
                "description": "Gmailì„ í†µí•´ ì´ë©”ì¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "to": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ìˆ˜ì‹ ì ì´ë©”ì¼ ì£¼ì†Œ ëª©ë¡"
                        },
                        "subject": {
                            "type": "string",
                            "description": "ì´ë©”ì¼ ì œëª©"
                        },
                        "body": {
                            "type": "string",
                            "description": "ì´ë©”ì¼ ë³¸ë¬¸"
                        },
                        "cc": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ì°¸ì¡° ì´ë©”ì¼ ì£¼ì†Œ ëª©ë¡"
                        }
                    },
                    "required": ["to", "subject", "body"]
                }
            }
        }
    ]


# ìº˜ë¦°ë” ì¼ì •ì„ í‘œ í˜•íƒœë¡œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
def format_calendar_events_as_table(events: List[Dict]) -> str:
    """ìº˜ë¦°ë” ì¼ì •ì„ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•íƒœë¡œ í¬ë§·íŒ…"""
    if not events:
        return "ì¡°íšŒëœ ì¼ì •ì´ ì—†ìŠµë‹ˆë‹¤."

    # í‘œ í—¤ë”
    table = "## ğŸ“… ì¼ì • ëª©ë¡\n\n"
    table += "| ë‚ ì§œ | ì‹œê°„ | ì œëª© | ì¥ì†Œ | ì„¤ëª… |\n"
    table += "|------|------|------|------|------|\n"

    for event in events:
        # ë‚ ì§œ/ì‹œê°„ íŒŒì‹±
        start_time = event.get('start', '')
        summary = event.get('summary', 'ì œëª© ì—†ìŒ')
        location = event.get('location', '-')
        description = event.get('description', '-')

        # ë‚ ì§œì™€ ì‹œê°„ ë¶„ë¦¬
        if 'T' in start_time:
            try:
                from datetime import datetime
                dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                date_str = dt.strftime('%m/%d')
                time_str = dt.strftime('%H:%M')

                # ì¢…ë£Œ ì‹œê°„ë„ íŒŒì‹±
                end_time = event.get('end', '')
                if 'T' in end_time:
                    end_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                    time_str += f" - {end_dt.strftime('%H:%M')}"
            except:
                date_str = start_time[:10] if len(start_time) >= 10 else start_time
                time_str = start_time[11:16] if len(start_time) > 16 else "-"
        else:
            date_str = start_time
            time_str = "ì¢…ì¼"

        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í‘œê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
        summary = (summary[:20] + "...") if len(summary) > 20 else summary
        location = (location[:15] + "...") if len(location) > 15 else location
        description = (description[:25] + "...") if len(description) > 25 else description

        # ë§ˆí¬ë‹¤ìš´ íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        summary = summary.replace('|', '\\|')
        location = location.replace('|', '\\|')
        description = description.replace('|', '\\|')

        table += f"| {date_str} | {time_str} | {summary} | {location} | {description} |\n"

    table += f"\nì´ **{len(events)}ê°œ**ì˜ ì¼ì •ì´ ìˆìŠµë‹ˆë‹¤."
    return table


# Google í•¨ìˆ˜ ì‹¤í–‰ í•¸ë“¤ëŸ¬
async def execute_google_function(function_name: str, arguments: dict):
    """Google í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜"""
    try:
        if not GOOGLE_SERVICES_AVAILABLE or not auth_service.is_authenticated():
            return {"error": "Google ì„œë¹„ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € Google ì¸ì¦ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”."}

        if function_name == "get_calendar_events":
            from datetime import datetime, timedelta

            # time_periodë¥¼ start_date, end_dateë¡œ ë³€í™˜
            time_period = arguments.get("time_period", "today")
            today = datetime.now().date()

            if time_period == "today":
                start_date = today.isoformat()
                end_date = today.isoformat()
            elif time_period == "tomorrow":
                tomorrow = today + timedelta(days=1)
                start_date = tomorrow.isoformat()
                end_date = tomorrow.isoformat()
            elif time_period == "this_week":
                # ì´ë²ˆ ì£¼ ì›”ìš”ì¼ë¶€í„° ì¼ìš”ì¼ê¹Œì§€
                days_since_monday = today.weekday()
                monday = today - timedelta(days=days_since_monday)
                sunday = monday + timedelta(days=6)
                start_date = monday.isoformat()
                end_date = sunday.isoformat()
            elif time_period == "next_week":
                # ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ë¶€í„° ì¼ìš”ì¼ê¹Œì§€
                days_since_monday = today.weekday()
                next_monday = today - timedelta(days=days_since_monday) + timedelta(days=7)
                next_sunday = next_monday + timedelta(days=6)
                start_date = next_monday.isoformat()
                end_date = next_sunday.isoformat()
            elif time_period == "this_month":
                # ì´ë²ˆ ë‹¬ 1ì¼ë¶€í„° ë§ì¼ê¹Œì§€
                first_day = today.replace(day=1)
                if today.month == 12:
                    last_day = today.replace(year=today.year + 1, month=1, day=1) - timedelta(days=1)
                else:
                    last_day = today.replace(month=today.month + 1, day=1) - timedelta(days=1)
                start_date = first_day.isoformat()
                end_date = last_day.isoformat()
            elif time_period == "next_month":
                # ë‹¤ìŒ ë‹¬ 1ì¼ë¶€í„° ë§ì¼ê¹Œì§€
                if today.month == 12:
                    next_month_first = today.replace(year=today.year + 1, month=1, day=1)
                    next_month_last = today.replace(year=today.year + 1, month=2, day=1) - timedelta(days=1)
                else:
                    next_month_first = today.replace(month=today.month + 1, day=1)
                    if today.month == 11:
                        next_month_last = today.replace(year=today.year + 1, month=1, day=1) - timedelta(days=1)
                    else:
                        next_month_last = today.replace(month=today.month + 2, day=1) - timedelta(days=1)
                start_date = next_month_first.isoformat()
                end_date = next_month_last.isoformat()
            else:
                # ê¸°ë³¸ê°’ì€ ì˜¤ëŠ˜
                start_date = today.isoformat()
                end_date = today.isoformat()

            result = await calendar_service.get_events(
                start_date=start_date,
                end_date=end_date,
                max_results=arguments.get("max_results", 10)
            )
            return result

        elif function_name == "create_calendar_event":
            from google_services import CalendarEvent
            event_data = CalendarEvent(
                summary=arguments["summary"],
                description=arguments.get("description", ""),
                start_datetime=arguments["start_datetime"],
                end_datetime=arguments["end_datetime"],
                attendees=arguments.get("attendees", [])
            )
            result = await calendar_service.create_event(event_data)
            return result

        elif function_name == "find_free_time":
            result = await calendar_service.find_free_time(
                duration_minutes=arguments["duration_minutes"],
                date_range=arguments["date_range"],
                working_hours_only=arguments.get("working_hours_only", True)
            )
            return result

        elif function_name == "get_emails":
            result = await gmail_service.get_messages(
                query=arguments.get("query", ""),
                max_results=arguments.get("max_results", 10)
            )
            return result

        elif function_name == "send_email":
            from google_services import EmailMessage
            email_data = EmailMessage(
                to=arguments["to"],
                subject=arguments["subject"],
                body=arguments["body"],
                cc=arguments.get("cc", [])
            )
            result = await gmail_service.send_email(email_data)
            return result

        else:
            return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {function_name}"}

    except Exception as e:
        print(f"Google function execution error: {e}")
        return {"error": f"í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}


# ê°œì„ ëœ í†µí•© API í•¨ìˆ˜ (Responses API + Assistant API)
async def create_response_with_best_api(
        session_id: str,
        model: str,
        instructions: str,
        user_input: str,
        conversation_messages: List[Dict],
        available_tools: List[Dict],
        needs_web_search: bool,
        model_config: Dict
) -> str:
    """
    ìµœì ì˜ OpenAI APIë¥¼ ì„ íƒí•˜ì—¬ ì‘ë‹µ ìƒì„±
    - Assistant API (ë³µì¡í•œ ëŒ€í™”, ë„êµ¬ ì‚¬ìš©)
    - Responses API (ì›¹ ê²€ìƒ‰, ê°„ë‹¨í•œ ë„êµ¬ ì‚¬ìš©)
    - Chat Completions API (í´ë°±)
    """

    # 1. Assistant API ì‚¬ìš© ì¡°ê±´ í™•ì¸
    use_assistant_api = (
            model_config.get("supports_assistant", False) and
            not needs_web_search  # ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° (ë„êµ¬ ìœ ë¬´ ë¬´ê´€)
    )

    print(f"ğŸ” API Selection Debug:")
    print(f"  - Model: {model}")
    print(f"  - supports_assistant: {model_config.get('supports_assistant', False)}")
    print(f"  - needs_web_search: {needs_web_search}")
    print(f"  - available_tools: {len(available_tools) if available_tools else 0}")
    print(f"  - use_assistant_api: {use_assistant_api}")

    if use_assistant_api:
        print("ğŸ¯ Using Assistant API for complex conversation with tools")
        return await create_response_with_assistant_api(
            session_id, user_input, model, model_config, instructions
        )

    # 2. Responses API ì‚¬ìš© (ê¸°ì¡´ ë¡œì§)
    return await create_response_with_responses_api_fallback(
        model, instructions, user_input, conversation_messages,
        available_tools, needs_web_search, model_config
    )


def build_context_input(user_input: str, conversation_messages: list) -> str:
    """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì…ë ¥ êµ¬ì„±"""
    context_input = user_input
    if conversation_messages:
        # ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨ (ìµœëŒ€ 5ê°œ)
        recent_messages = conversation_messages[-5:]
        context_parts = []
        for msg in recent_messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'user':
                context_parts.append(f"ì‚¬ìš©ì: {content}")
            else:
                context_parts.append(f"AI: {content}")
        
        # ì»¨í…ìŠ¤íŠ¸ì™€ í˜„ì¬ ì§ˆë¬¸ì„ ê²°í•©
        context_input = f"**ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸:**\n" + "\n".join(context_parts) + f"\n\n**í˜„ì¬ ì§ˆë¬¸:** {user_input}"
    
    return context_input


# ê¸°ì¡´ Responses API í•¨ìˆ˜ (ì´ë¦„ ë³€ê²½)
async def create_response_with_responses_api_fallback(
        model: str,
        instructions: str,
        user_input: str,
        conversation_messages: List[Dict],
        available_tools: List[Dict],
        needs_web_search: bool,
        model_config: Dict
) -> str:
    """
    Responses APIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (Assistant API í´ë°±)
    Google Functionsì™€ ì›¹ ê²€ìƒ‰ì„ ì§€ì›
    ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨
    """

    print(f"ğŸ” create_response_with_responses_api_fallback called with:")
    print(f"  - Model: {model}")
    print(f"  - Instructions present: {bool(instructions)}")
    print(f"  - User input: {user_input}")
    print(f"  - Conversation messages: {len(conversation_messages)}")
    print(f"  - Available tools: {len(available_tools)}")
    print(f"  - Needs web search: {needs_web_search}")
    print(f"  - Model config: {model_config}")

    # 1. ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
    if needs_web_search and model_config.get("supports_web_search", False):
        print("ğŸ” Using Responses API with web search")

        async def web_search_call():
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì…ë ¥ êµ¬ì„±
            context_input = build_context_input(user_input, conversation_messages)
            
            return await client.responses.create(
                model=model,
                instructions=instructions,
                input=context_input,
                tools=[{"type": "web_search"}]
            )

        response = await safe_openai_call_with_retry(web_search_call, user_content=user_input)

        if isinstance(response, dict) and "error" in response:
            return response["error"]

        return extract_response_content(response, include_sources=True)

    # 2. Google ë„êµ¬ê°€ í•„ìš”í•œ ê²½ìš°
    elif available_tools:
        print("ğŸ› ï¸ Using Responses API with Google tools")

        # Google ë„êµ¬ë¥¼ Responses API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        responses_tools = convert_tools_for_responses_api(available_tools)

        async def tools_call():
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì…ë ¥ êµ¬ì„±
            context_input = build_context_input(user_input, conversation_messages)
            
            return await client.responses.create(
                model=model,
                instructions=instructions,
                input=context_input,
                tools=responses_tools
            )

        response = await safe_openai_call_with_retry(tools_call, user_content=user_input)

        if isinstance(response, dict) and "error" in response:
            return response["error"]

        return await process_tool_calls_in_response(response)

    # 3. ì¼ë°˜ ëŒ€í™”
    else:
        print("ğŸ’¬ Using Responses API for general conversation")

        async def general_call():
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì…ë ¥ êµ¬ì„±
            context_input = build_context_input(user_input, conversation_messages)
            
            return await client.responses.create(
                model=model,
                instructions=instructions,
                input=context_input
            )

        response = await safe_openai_call_with_retry(general_call, user_content=user_input)

        if isinstance(response, dict) and "error" in response:
            # Responses API ì™„ì „ ì‹¤íŒ¨ ì‹œ Chat Completionsë¡œ í´ë°±
            print("âš ï¸ Responses API completely failed, trying Chat Completions fallback")
            return await safe_fallback_to_chat_completions(
                model, conversation_messages, available_tools, model_config, user_input
            )

        return extract_response_content(response)


def extract_response_content(response, include_sources: bool = False) -> str:
    """Responses API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ"""
    content = ""
    sources = []

    for output_item in response.output:
        if output_item.type == 'message' and hasattr(output_item, 'content'):
            for content_item in output_item.content:
                if content_item.type == 'output_text':
                    # í…ìŠ¤íŠ¸ ë‚´ìš© ì •ê·œí™” - ì¤„ë°”ê¿ˆ ì²˜ë¦¬ ê°œì„ 
                    raw_text = content_item.text
                    
                    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ê·œí™”í•˜ë˜, ì¤„ë°”ê¿ˆì€ ë³´ì¡´
                    import re
                    # ë¨¼ì € \r\nì„ \nìœ¼ë¡œ í†µì¼
                    normalized_text = raw_text.replace('\r\n', '\n').replace('\r', '\n')
                    
                    # ì¤„ë°”ê¿ˆ ë¬¸ìëŠ” ë³´ì¡´í•˜ë©´ì„œ ì—°ì†ëœ ê³µë°±ë§Œ ì •ë¦¬
                    # ë‹¨, ì¤„ ëì˜ ê³µë°±ì€ ì œê±°í•˜ê³  ì¤„ë°”ê¿ˆì€ ìœ ì§€
                    lines = normalized_text.split('\n')
                    processed_lines = []
                    
                    for line in lines:
                        # ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°í•˜ê³  ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
                        cleaned_line = re.sub(r'\s+', ' ', line.strip())
                        processed_lines.append(cleaned_line)
                    
                    # ë¹ˆ ì¤„ë„ ë³´ì¡´í•˜ë©´ì„œ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
                    content += '\n'.join(processed_lines)

                    # ì›¹ ê²€ìƒ‰ ì†ŒìŠ¤ ì¶”ì¶œ
                    if include_sources and hasattr(content_item, 'annotations'):
                        for annotation in content_item.annotations:
                            if annotation.type == 'url_citation':
                                sources.append({
                                    'title': getattr(annotation, 'title', ''),
                                    'url': getattr(annotation, 'url', ''),
                                })

    # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€ - ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ê°œì„ 
    if include_sources and sources:
        content += "\n\n## ì°¸ê³  ì¶œì²˜\n\n"
        for i, source in enumerate(sources, 1):
            title = source['title'] if source['title'] else f"ì¶œì²˜ {i}"
            # ê° ë¦¬ìŠ¤íŠ¸ í•­ëª© ë’¤ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆ ì¶”ê°€ (ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ í¬ë§·)
            content += f"{i}. **[{title}]({source['url']})**\n\n"
        print(f"ğŸ“š Found {len(sources)} web search sources")

    return content.strip()  # ë§ˆì§€ë§‰ì— ë¶ˆí•„ìš”í•œ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°


def convert_tools_for_responses_api(chat_tools: List[Dict]) -> List[Dict]:
    """Chat Completions API ë„êµ¬ë¥¼ Responses API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    responses_tools = []

    for tool in chat_tools:
        if tool.get("type") == "function":
            # Google í•¨ìˆ˜ë“¤ì„ Responses API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            responses_tools.append({
                "type": "function",
                "function": tool["function"]
            })

    return responses_tools


async def process_tool_calls_in_response(response) -> str:
    """Responses APIì—ì„œ ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ ì²˜ë¦¬"""
    content = ""

    for output_item in response.output:
        if output_item.type == 'message' and hasattr(output_item, 'content'):
            for content_item in output_item.content:
                if content_item.type == 'output_text':
                    content += content_item.text
                elif content_item.type == 'tool_call':
                    # ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
                    function_name = content_item.function.name
                    function_args = json.loads(content_item.function.arguments)

                    print(f"ğŸ”§ Executing tool: {function_name}({function_args})")
                    result = await execute_google_function(function_name, function_args)

                    # ê²°ê³¼ í¬ë§·íŒ…
                    if function_name == "get_calendar_events" and isinstance(result, list):
                        content += "\n\n" + format_calendar_events_as_table(result)
                    else:
                        content += f"\n\n## ğŸ”§ {function_name} ì‹¤í–‰ ê²°ê³¼\n\n"
                        if isinstance(result, (dict, list)):
                            content += f"```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```"
                        else:
                            content += str(result)

    return content


async def fallback_to_chat_completions(
        model: str,
        messages: List[Dict],
        tools: List[Dict],
        model_config: Dict
) -> str:
    """Chat Completions APIë¡œ í´ë°± (ê¸°ì¡´ ë²„ì „)"""
    print("âš ï¸ Falling back to Chat Completions API")

    chat_params = {
        "model": model,
        "messages": messages,
        "max_tokens": model_config["max_tokens"],
        "temperature": model_config["temperature"]
    }

    if tools:
        chat_params["tools"] = tools
        chat_params["tool_choice"] = "auto"

    response = await client.chat.completions.create(**chat_params)

    # Function calls ì²˜ë¦¬
    if response.choices[0].message.tool_calls:
        content = response.choices[0].message.content or ""

        for tool_call in response.choices[0].message.tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)

            result = await execute_google_function(function_name, function_args)

            if function_name == "get_calendar_events" and isinstance(result, list):
                content += "\n\n" + format_calendar_events_as_table(result)
            else:
                content += f"\n\n## ğŸ”§ {function_name} ì‹¤í–‰ ê²°ê³¼\n\n"
                if isinstance(result, (dict, list)):
                    content += f"```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```"
                else:
                    content += str(result)

        return content

    return response.choices[0].message.content


async def safe_fallback_to_chat_completions(
        model: str,
        messages: List[Dict],
        tools: List[Dict],
        model_config: Dict,
        user_content: str
) -> str:
    """ì•ˆì „í•œ Chat Completions API í´ë°± (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
    print("ğŸ›¡ï¸ Safe fallback to Chat Completions API")

    async def chat_call():
        chat_params = {
            "model": model,
            "messages": messages,
            "max_tokens": model_config["max_tokens"],
            "temperature": model_config["temperature"]
        }

        if tools:
            chat_params["tools"] = tools
            chat_params["tool_choice"] = "auto"

        return await client.chat.completions.create(**chat_params)

    response = await safe_openai_call_with_retry(
        chat_call,
        max_retries=2,  # í´ë°±ì´ë¯€ë¡œ ì¬ì‹œë„ íšŸìˆ˜ ì¤„ì„
        user_content=user_content
    )

    if isinstance(response, dict) and "error" in response:
        return response["error"]

    # Function calls ì²˜ë¦¬
    if response.choices[0].message.tool_calls:
        content = response.choices[0].message.content or ""

        for tool_call in response.choices[0].message.tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)

            result = await execute_google_function(function_name, function_args)

            if function_name == "get_calendar_events" and isinstance(result, list):
                content += "\n\n" + format_calendar_events_as_table(result)
            else:
                content += f"\n\n## ğŸ”§ {function_name} ì‹¤í–‰ ê²°ê³¼\n\n"
                if isinstance(result, (dict, list)):
                    content += f"```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```"
                else:
                    content += str(result)

        return content

    return response.choices[0].message.content


# Assistant API ê´€ë ¨ í•¨ìˆ˜ë“¤
async def get_or_create_assistant(session_id: str, model: str, instructions: str = None) -> str:
    """ì„¸ì…˜ìš© Assistantë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±"""

    # ì´ë¯¸ Assistantê°€ ìˆëŠ”ì§€ í™•ì¸
    if session_id in assistants_db:
        assistant_id = assistants_db[session_id]
        try:
            # Assistant ì¡´ì¬ í™•ì¸
            assistant = await client.beta.assistants.retrieve(assistant_id)
            print(f"ğŸ¤– Using existing assistant: {assistant_id}")
            return assistant_id
        except Exception as e:
            print(f"âš ï¸ Existing assistant not found: {e}")
            # Assistantê°€ ì‚­ì œë˜ì—ˆë‹¤ë©´ ìƒˆë¡œ ìƒì„±
            del assistants_db[session_id]

    # ìƒˆ Assistant ìƒì„±
    try:
        print(f"ğŸ†• Creating new assistant for session: {session_id}")

        # Google ë„êµ¬ë“¤ì„ Assistant í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        tools = []
        if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
            tools.extend(get_google_tools())
            print(f"ğŸ› ï¸ Added {len(tools)} Google tools to assistant")

        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸° (ì„ íƒì )
        tool_resources = {}
        try:
            vector_store_id = await create_or_get_vector_store(session_id)
            if vector_store_id:
                tool_resources["file_search"] = {
                    "vector_store_ids": [vector_store_id]
                }
                # íŒŒì¼ ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€
                tools.append({"type": "file_search"})
                print(f"ğŸ—‚ï¸ Added vector store to assistant: {vector_store_id}")
        except Exception as vs_error:
            print(f"âš ï¸ Vector store creation failed, continuing without: {vs_error}")

        # instructionsê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not instructions:
            instructions = """ë‹¹ì‹ ì€ NSales Proì˜ ì „ë¬¸ì ì¸ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 

ì£¼ìš” ì—­í• :
- ì˜ì—… ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ
- í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ ë° ê´€ë¦¬ ì§€ì›
- ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ì „ë¬¸ì  ë‹µë³€
- Google Calendar ë° Gmail í†µí•© ê¸°ëŠ¥ í™œìš©

ì§€ì¹¨:
- í•­ìƒ í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë¬¸ë§¥ì„ ìœ ì§€í•˜ì„¸ìš”
- Google ì„œë¹„ìŠ¤ ë„êµ¬ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œê³µí•˜ì„¸ìš”
- ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° ì›¹ ê²€ìƒ‰ì„ í™œìš©í•˜ì„¸ìš”

Google ì„œë¹„ìŠ¤ ë©˜ì…˜:
- @ìº˜ë¦°ë” â†’ ìº˜ë¦°ë” ì¼ì • ì¡°íšŒ
- @ë©”ì¼ â†’ ì´ë©”ì¼ ì¡°íšŒ/ë°œì†¡  
- @ì¼ì •ìƒì„± â†’ ìƒˆ ì¼ì • ìƒì„±
- @ë¹ˆì‹œê°„ â†’ ë¹ˆ ì‹œê°„ ê²€ìƒ‰"""

        # Assistant ìƒì„± (ë²¡í„° ìŠ¤í† ì–´ ë¦¬ì†ŒìŠ¤ í¬í•¨)
        assistant_params = {
            "name": f"NSales Pro Assistant - {session_id[:8]}",
            "instructions": instructions,
            "model": model,
            "tools": tools
        }

        # ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆëŠ” ê²½ìš° tool_resources ì¶”ê°€
        if tool_resources:
            assistant_params["tool_resources"] = tool_resources

        assistant = await client.beta.assistants.create(**assistant_params)

        assistant_id = assistant.id
        assistants_db[session_id] = assistant_id
        print(f"âœ… Created assistant: {assistant_id}")

        return assistant_id

    except Exception as e:
        print(f"ğŸš¨ Failed to create assistant: {e}")
        raise e


async def get_or_create_thread(session_id: str) -> str:
    """ì„¸ì…˜ìš© Threadë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•˜ê³  ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë™ê¸°í™”"""

    # ì´ë¯¸ Threadê°€ ìˆëŠ”ì§€ í™•ì¸
    if session_id in threads_db:
        thread_id = threads_db[session_id]
        try:
            # Thread ì¡´ì¬ í™•ì¸
            thread = await client.beta.threads.retrieve(thread_id)
            print(f"ğŸ§µ Using existing thread: {thread_id}")
            return thread_id
        except Exception as e:
            print(f"âš ï¸ Existing thread not found: {e}")
            # Threadê°€ ì‚­ì œë˜ì—ˆë‹¤ë©´ ìƒˆë¡œ ìƒì„±
            del threads_db[session_id]

    # ìƒˆ Thread ìƒì„±
    try:
        print(f"ğŸ†• Creating new thread for session: {session_id}")

        # ê¸°ì¡´ ì„¸ì…˜ ë©”ì‹œì§€ë“¤ì„ Threadì— ì¶”ê°€í•  ë©”ì‹œì§€ë¡œ ì¤€ë¹„
        initial_messages = []
        session_messages = messages_db.get(session_id, [])

        # ìµœê·¼ 20ê°œ ë©”ì‹œì§€ë§Œ Threadì— í¬í•¨ (í† í° ì ˆì•½)
        recent_messages = session_messages[-20:] if len(session_messages) > 20 else session_messages

        for msg in recent_messages:
            # Assistant API ThreadëŠ” system ë©”ì‹œì§€ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œì™¸
            if msg.get("role") in ["user", "assistant"]:
                initial_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        # Thread ìƒì„± (ê¸°ì¡´ ë©”ì‹œì§€ í¬í•¨)
        if initial_messages:
            thread = await client.beta.threads.create(messages=initial_messages)
            print(f"ğŸ“š Thread created with {len(initial_messages)} existing messages")
        else:
            thread = await client.beta.threads.create()
            print(f"ğŸ“ Empty thread created")

        thread_id = thread.id
        threads_db[session_id] = thread_id
        print(f"âœ… Created thread: {thread_id}")

        return thread_id

    except Exception as e:
        print(f"ğŸš¨ Failed to create thread: {e}")
        raise e


async def create_response_with_assistant_api(
        session_id: str,
        user_input: str,
        model: str,
        model_config: Dict,
        instructions: str = None
) -> str:
    """Assistant APIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""

    try:
        print(f"ğŸ¯ Using Assistant API for session: {session_id}")
        print(f"ğŸ” Session messages count: {len(messages_db.get(session_id, []))}")

        # Assistantì™€ Thread ì¤€ë¹„
        assistant_id = await get_or_create_assistant(session_id, model, instructions)
        thread_id = await get_or_create_thread(session_id)

        print(f"ğŸ“ Assistant ID: {assistant_id}")
        print(f"ğŸ§µ Thread ID: {thread_id}")

        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ Threadì— ì¶”ê°€
        await client.beta.threads.messages.create(
            thread_id=thread_id,
            role="user",
            content=user_input
        )

        # Run ìƒì„± ë° ì‹¤í–‰
        async def assistant_call():
            return await client.beta.threads.runs.create_and_poll(
                thread_id=thread_id,
                assistant_id=assistant_id,
                timeout=60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
            )

        run = await safe_openai_call_with_retry(assistant_call, user_content=user_input)

        if isinstance(run, dict) and "error" in run:
            return run["error"]

        # Run ìƒíƒœ í™•ì¸
        if run.status == 'completed':
            # ìµœì‹  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
            messages = await client.beta.threads.messages.list(
                thread_id=thread_id,
                order="desc",
                limit=1
            )

            if messages.data:
                latest_message = messages.data[0]
                if latest_message.role == "assistant" and latest_message.content:
                    # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                    content = ""
                    for content_block in latest_message.content:
                        if content_block.type == "text":
                            content += content_block.text.value

                    print(f"âœ… Assistant response completed")
                    return content

        elif run.status == 'requires_action':
            # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
            print(f"ğŸ”§ Assistant requires action: tool calls")
            return await handle_assistant_tool_calls(run, thread_id)

        elif run.status in ['failed', 'expired', 'cancelled']:
            error_msg = f"Assistant ì‹¤í–‰ ì‹¤íŒ¨: {run.status}"
            if hasattr(run, 'last_error') and run.last_error:
                error_msg += f" - {run.last_error.message}"
            return error_msg

        else:
            return f"Assistant ì‹¤í–‰ ìƒíƒœ: {run.status}. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    except Exception as e:
        return handle_openai_error(e, user_content=user_input)


async def handle_assistant_tool_calls(run, thread_id: str) -> str:
    """Assistantì˜ ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""

    try:
        tool_outputs = []

        for tool_call in run.required_action.submit_tool_outputs.tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)

            print(f"ğŸ”§ Assistant tool call: {function_name}({function_args})")

            # Google í•¨ìˆ˜ ì‹¤í–‰
            result = await execute_google_function(function_name, function_args)

            # ê²°ê³¼ í¬ë§·íŒ…
            if function_name == "get_calendar_events" and isinstance(result, list):
                output = format_calendar_events_as_table(result)
            else:
                output = json.dumps(result, ensure_ascii=False, indent=2)

            tool_outputs.append({
                "tool_call_id": tool_call.id,
                "output": output
            })

        # ë„êµ¬ ì¶œë ¥ ì œì¶œ ë° Run ì™„ë£Œ ëŒ€ê¸°
        completed_run = await client.beta.threads.runs.submit_tool_outputs_and_poll(
            thread_id=thread_id,
            run_id=run.id,
            tool_outputs=tool_outputs
        )

        if completed_run.status == 'completed':
            # ìµœì‹  ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
            messages = await client.beta.threads.messages.list(
                thread_id=thread_id,
                order="desc",
                limit=1
            )

            if messages.data:
                latest_message = messages.data[0]
                if latest_message.role == "assistant" and latest_message.content:
                    content = ""
                    for content_block in latest_message.content:
                        if content_block.type == "text":
                            content += content_block.text.value

                    print(f"âœ… Assistant tool calls completed")
                    return content

        # ìµœì¢… ì‘ë‹µ ë°˜í™˜
        final_content = ""
        if completed_run.status == 'completed':
            messages = await client.beta.threads.messages.list(
                thread_id=thread_id,
                order="desc",
                limit=1
            )
            if messages.data and messages.data[0].content:
                for content_block in messages.data[0].content:
                    if content_block.type == "text":
                        final_content += content_block.text.value

        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
        tool_results_summary = "\n".join([f"- {tool['tool_call_id']}: {tool['output'][:100]}..." for tool in tool_outputs])

        # ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ AIì˜ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        if final_content:
            # AI ì‘ë‹µê³¼ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í•¨ê»˜ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # return f"{final_content}\n\n--- ë„êµ¬ ì‹¤í–‰ ìš”ì•½ ---\n{tool_results_summary}"
            return final_content
        else:
            # ë§Œì•½ AIì˜ ìµœì¢… ë‹µë³€ì´ ì—†ë‹¤ë©´, ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¼ë„ ë°˜í™˜í•©ë‹ˆë‹¤.
            return f"ë„êµ¬ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìš”ì•½:\n{tool_results_summary}"

    except Exception as e:
        print(f"ğŸš¨ Assistant tool call error: {e}")
        return f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# ì²´ê³„ì ì¸ OpenAI ì—ëŸ¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def handle_openai_error(e: Exception, user_content: str = "", request_id: str = None) -> str:
    """
    OpenAI API ì—ëŸ¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì‚¬ìš©ì ì¹œí™”ì ì¸ ë©”ì‹œì§€ ë°˜í™˜
    """
    print(f"ğŸš¨ OpenAI API Error: {type(e).__name__}: {e}")

    # Request ID ë¡œê¹… (ë””ë²„ê¹…ìš©)
    if request_id:
        print(f"ğŸ” Request ID: {request_id}")
    elif hasattr(e, 'request_id'):
        print(f"ğŸ” Request ID: {e.request_id}")

    if isinstance(e, openai.APIConnectionError):
        return handle_connection_error(e)
    elif isinstance(e, openai.RateLimitError):
        return handle_rate_limit_error(e)
    elif isinstance(e, openai.AuthenticationError):
        return handle_auth_error(e)
    elif isinstance(e, openai.PermissionDeniedError):
        return handle_permission_error(e)
    elif isinstance(e, openai.NotFoundError):
        return handle_not_found_error(e)
    elif isinstance(e, openai.UnprocessableEntityError):
        return handle_validation_error(e, user_content)
    elif isinstance(e, openai.InternalServerError):
        return handle_server_error(e)
    elif isinstance(e, openai.BadRequestError):
        return handle_bad_request_error(e, user_content)
    else:
        return handle_generic_error(e, user_content)


def handle_connection_error(e: openai.APIConnectionError) -> str:
    """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"ğŸŒ Connection Error: {e}")
    return "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def handle_rate_limit_error(e: openai.RateLimitError) -> str:
    """API ì†ë„ ì œí•œ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"â±ï¸ Rate Limit Error: {e}")
    return "í˜„ì¬ ìš”ì²­ì´ ë§ì•„ ì²˜ë¦¬ê°€ ì§€ì—°ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def handle_auth_error(e: openai.AuthenticationError) -> str:
    """ì¸ì¦ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"ğŸ” Authentication Error: {e}")
    return "AI ì„œë¹„ìŠ¤ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."


def handle_permission_error(e: openai.PermissionDeniedError) -> str:
    """ê¶Œí•œ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"ğŸš« Permission Error: {e}")
    return "AI ì„œë¹„ìŠ¤ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."


def handle_not_found_error(e: openai.NotFoundError) -> str:
    """ë¦¬ì†ŒìŠ¤ ì—†ìŒ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"â“ Not Found Error: {e}")
    return "ìš”ì²­í•œ AI ëª¨ë¸ì´ë‚˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."


def handle_validation_error(e: openai.UnprocessableEntityError, user_content: str) -> str:
    """ì…ë ¥ ê²€ì¦ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"âš ï¸ Validation Error: {e}")

    error_message = str(e).lower()
    if "context_length_exceeded" in error_message or "maximum context length" in error_message:
        return "ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì§§ì€ ë©”ì‹œì§€ë¡œ ë‚˜ëˆ„ì–´ ë³´ë‚´ì£¼ì„¸ìš”."
    elif "invalid_request" in error_message:
        return "ìš”ì²­ í˜•ì‹ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    else:
        return f"ì…ë ¥ ë‚´ìš©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤: {user_content[:50]}{'...' if len(user_content) > 50 else ''}"


def handle_server_error(e: openai.InternalServerError) -> str:
    """ì„œë²„ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"ğŸ”¥ Server Error: {e}")
    return "AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def handle_bad_request_error(e: openai.BadRequestError, user_content: str) -> str:
    """ì˜ëª»ëœ ìš”ì²­ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"âŒ Bad Request Error: {e}")

    error_message = str(e).lower()
    if "safety" in error_message or "policy" in error_message:
        return "ìš”ì²­í•œ ë‚´ìš©ì´ AI ì‚¬ìš© ì •ì±…ì— ìœ„ë°°ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    elif "model" in error_message:
        return "ì„ íƒí•œ AI ëª¨ë¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
    else:
        return "ìš”ì²­ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‚´ìš©ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def handle_generic_error(e: Exception, user_content: str) -> str:
    """ì¼ë°˜ ì—ëŸ¬ ì²˜ë¦¬"""
    print(f"ğŸ” Generic Error: {type(e).__name__}: {e}")
    return f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


async def safe_openai_call_with_retry(
        api_call_func,
        max_retries: int = 3,
        base_delay: float = 1.0,
        user_content: str = ""
):
    """
    ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì•ˆì „í•œ OpenAI API í˜¸ì¶œ
    """
    last_exception = None

    for attempt in range(max_retries):
        try:
            # API í˜¸ì¶œ ì‹œë„
            response = await api_call_func()

            # Request ID ì¶”ì¶œ ë° ë¡œê¹…
            if hasattr(response, '_request_id'):
                print(f"âœ… OpenAI Request ID: {response._request_id}")

            return response

        except (openai.RateLimitError, openai.APIConnectionError, openai.InternalServerError) as e:
            last_exception = e

            # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ë“¤
            if attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                print(f"ğŸ”„ Retry {attempt + 1}/{max_retries} after {delay}s due to: {type(e).__name__}")
                await asyncio.sleep(delay)
            else:
                print(f"âŒ Max retries exceeded for {type(e).__name__}")

        except Exception as e:
            # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬ë“¤
            last_exception = e
            print(f"ğŸ’¥ Non-retryable error: {type(e).__name__}: {e}")
            break

    # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì²˜ë¦¬
    return {"error": handle_openai_error(last_exception, user_content)}


# Pydantic ëª¨ë¸ë“¤
class ChatMessage(BaseModel):
    id: str
    content: str
    role: str  # 'user' or 'assistant'
    timestamp: datetime
    sessionId: str


class ChatRequest(BaseModel):
    content: str
    sessionId: str
    model: Optional[str] = "gpt-4o"  # ê¸°ë³¸ê°’ì€ gpt-4o
    webSearch: Optional[bool] = False  # ì›¹ ê²€ìƒ‰ ì—¬ë¶€


class ChatResponse(BaseModel):
    id: str
    content: str
    role: str
    timestamp: datetime
    sessionId: str


class ChatSession(BaseModel):
    id: str
    title: str
    createdAt: datetime
    updatedAt: datetime
    messageCount: int
    titleGenerated: Optional[bool] = False
    titleGeneratedAt: Optional[datetime] = None


class SessionCreateRequest(BaseModel):
    title: Optional[str] = None


class SessionUpdateRequest(BaseModel):
    title: str


class ChatSearch(BaseModel):
    query: Optional[str] = None
    page: int = 0
    size: int = 50
    sort: Optional[str] = None


class ChatSessionList(BaseModel):
    sessions: List[ChatSession]
    totalElements: int
    totalPages: int
    currentPage: int
    size: int


class ChatHistory(BaseModel):
    messages: List[ChatMessage]
    sessionId: str
    totalCount: int


class ChatStreamChunk(BaseModel):
    id: str
    content: str
    role: str
    timestamp: datetime
    sessionId: str
    isComplete: bool = False
    functionCall: Optional[str] = None  # Function name if this chunk is from function execution
    functionStatus: Optional[str] = None  # 'running', 'completed', 'error'


# ğŸ“ ê°œì„ ëœ íŒŒì¼ ì²˜ë¦¬ ì‹œìŠ¤í…œ (OpenAI Files API + ë¡œì»¬ í´ë°±)
async def process_file_with_openai(file_content: bytes, filename: str, content_type: str, session_id: str = None,
                                   add_to_vector_store: bool = False) -> str:
    """OpenAI Files APIë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ íŒŒì¼ ì²˜ë¦¬ (ë²¡í„° ìŠ¤í† ì–´ í†µí•©)"""
    try:
        print(f"ğŸ” Processing file with OpenAI: {filename} ({content_type})")

        # OpenAI Files APIì— íŒŒì¼ ì—…ë¡œë“œ
        with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{filename}") as temp_file:
            temp_file.write(file_content)
            temp_file.flush()

            # OpenAI Files API ì—…ë¡œë“œ
            file_object = await client.files.create(
                file=open(temp_file.name, "rb"),
                purpose="assistants"  # ë¬¸ì„œ ë¶„ì„ìš©
            )

            print(f"âœ… File uploaded to OpenAI: {file_object.id}")

            # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
            await client.files.wait_for_processing(file_object.id)

            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ (ì„ íƒì )
            if add_to_vector_store and session_id:
                try:
                    vector_store_id = await create_or_get_vector_store(session_id)
                    if await add_file_to_vector_store(vector_store_id, file_object.id):
                        print(f"ğŸ“š File added to vector store for future reference")
                        # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ëœ ê²½ìš° íŒŒì¼ì„ ì‚­ì œí•˜ì§€ ì•ŠìŒ
                        file_should_be_deleted = False
                    else:
                        file_should_be_deleted = True
                except Exception as vs_error:
                    print(f"âš ï¸ Failed to add file to vector store: {vs_error}")
                    file_should_be_deleted = True
            else:
                file_should_be_deleted = True

            # Assistant APIë¥¼ í†µí•´ íŒŒì¼ ë¶„ì„
            analysis_result = await analyze_file_with_assistant(file_object.id, filename)

            # íŒŒì¼ ì •ë¦¬ (ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
            if file_should_be_deleted:
                try:
                    await client.files.delete(file_object.id)
                    print(f"ğŸ—‘ï¸ Cleaned up file: {file_object.id}")
                except:
                    pass  # ì‚­ì œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(temp_file.name)

            return analysis_result

    except Exception as e:
        print(f"ğŸš¨ OpenAI Files API error: {e}")
        # í´ë°±: ë¡œì»¬ ì²˜ë¦¬
        return await process_file_locally(file_content, filename, content_type)


async def analyze_file_with_assistant(file_id: str, filename: str) -> str:
    """Assistant APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë¶„ì„"""
    try:
        # ì„ì‹œ Assistant ìƒì„± (íŒŒì¼ ë¶„ì„ ì „ìš©)
        assistant = await client.beta.assistants.create(
            name="Document Analyzer",
            instructions="""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¬¸ì„œ ë¶„ì„ AIì…ë‹ˆë‹¤. 
            
ì—…ë¬´:
- ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ê³  ë¶„ì„
- í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì„œ ëª¨ë‘ ì²˜ë¦¬ ê°€ëŠ¥
- ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©, êµ¬ì¡°, ì¤‘ìš” ì •ë³´ë¥¼ ìš”ì•½

ì‘ë‹µ í˜•ì‹:
1. ğŸ“„ ë¬¸ì„œ ìš”ì•½: ì£¼ìš” ë‚´ìš© ìš”ì•½
2. ğŸ“‹ í•µì‹¬ ì •ë³´: ì¤‘ìš”í•œ ë°ì´í„°, ìˆ˜ì¹˜, ë‚ ì§œ ë“±
3. ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸: ì›ë³¸ í…ìŠ¤íŠ¸ (êµ¬ì¡°í™”ëœ í˜•íƒœ)

í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.""",
            model="gpt-4o",
            tools=[{"type": "file_search"}]
        )

        # Thread ìƒì„±
        thread = await client.beta.threads.create()

        # íŒŒì¼ê³¼ í•¨ê»˜ ë©”ì‹œì§€ ìƒì„±
        message = await client.beta.threads.messages.create(
            thread_id=thread.id,
            role="user",
            content=f"ë‹¤ìŒ íŒŒì¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {filename}",
            attachments=[{
                "file_id": file_id,
                "tools": [{"type": "file_search"}]
            }]
        )

        # Assistant ì‹¤í–‰
        run = await client.beta.threads.runs.create_and_poll(
            thread_id=thread.id,
            assistant_id=assistant.id,
            timeout=60
        )

        if run.status == 'completed':
            # ì‘ë‹µ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
            messages = await client.beta.threads.messages.list(
                thread_id=thread.id,
                order="desc",
                limit=1
            )

            if messages.data:
                response_content = ""
                for content_block in messages.data[0].content:
                    if content_block.type == "text":
                        response_content += content_block.text.value

                # ì •ë¦¬
                await client.beta.assistants.delete(assistant.id)

                return response_content

        # ì‹¤íŒ¨ ì‹œ í´ë°±
        await client.beta.assistants.delete(assistant.id)
        raise Exception(f"Assistant API run failed: {run.status}")

    except Exception as e:
        print(f"ğŸš¨ Assistant file analysis error: {e}")
        raise e


async def process_file_locally(file_content: bytes, filename: str, content_type: str) -> str:
    """ë¡œì»¬ íŒŒì¼ ì²˜ë¦¬ (í´ë°±)"""
    print(f"ğŸ”„ Fallback to local processing: {filename}")

    try:
        if content_type == "application/pdf" or filename.lower().endswith('.pdf'):
            return await extract_text_from_pdf_local(file_content)
        elif content_type in [
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document"] or filename.lower().endswith(
            '.docx'):
            return await extract_text_from_docx_local(file_content)
        elif content_type.startswith('image/'):
            return await process_image_with_hybrid_approach(file_content, filename)
        elif content_type == "text/plain" or filename.lower().endswith('.txt'):
            return file_content.decode('utf-8', errors='ignore')
        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {content_type}"
    except Exception as e:
        return f"ë¡œì»¬ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"


# ë¡œì»¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ì´ë¦„ ë³€ê²½)
async def extract_text_from_pdf_local(file_content: bytes) -> str:
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¡œì»¬)"""
    try:
        pdf_file = io.BytesIO(file_content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return f"ğŸ“„ PDF ë¬¸ì„œ ë‚´ìš©:\n\n{text.strip()}"
    except Exception as e:
        return f"PDF ì½ê¸° ì˜¤ë¥˜: {str(e)}"


async def extract_text_from_docx_local(file_content: bytes) -> str:
    """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¡œì»¬)"""
    try:
        doc_file = io.BytesIO(file_content)
        doc = docx.Document(doc_file)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return f"ğŸ“„ Word ë¬¸ì„œ ë‚´ìš©:\n\n{text.strip()}"
    except Exception as e:
        return f"DOCX ì½ê¸° ì˜¤ë¥˜: {str(e)}"


def encode_image_to_base64(file_content: bytes) -> str:
    """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
    return base64.b64encode(file_content).decode('utf-8')


async def analyze_image_with_gpt4o_vision(file_content: bytes, filename: str, prompt: str = None) -> str:
    """GPT-4o Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„"""
    try:
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        base64_image = encode_image_to_base64(file_content)
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if not prompt:
            prompt = """ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ì£¼ìš” ë‚´ìš©ê³¼ ê°ì²´ë“¤
2. í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ëª¨ë“  í…ìŠ¤íŠ¸ ë‚´ìš©
3. ë¬¸ì„œë‚˜ í‘œê°€ ìˆë‹¤ë©´ êµ¬ì¡°ì™€ ë°ì´í„°
4. ì „ì²´ì ì¸ ë§¥ë½ê³¼ ì˜ë¯¸
5. ë¹„ì¦ˆë‹ˆìŠ¤ë‚˜ ì—…ë¬´ì™€ ê´€ë ¨ëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ìƒì„¸íˆ ì„¤ëª…

í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        # GPT-4o Vision API í˜¸ì¶œ
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                                "detail": "high"  # ê³ í•´ìƒë„ ë¶„ì„
                            }
                        }
                    ]
                }
            ],
            max_tokens=2000,
            temperature=0.1  # ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
        )
        
        vision_result = response.choices[0].message.content
        return f"ğŸ” GPT-4o Vision ë¶„ì„ ê²°ê³¼:\n\n{vision_result}"
        
    except Exception as e:
        print(f"GPT-4o Vision API ì˜¤ë¥˜: {e}")
        return f"GPT-4o Vision ë¶„ì„ ì˜¤ë¥˜: {str(e)}"


async def extract_text_from_image_local(file_content: bytes) -> str:
    """ì´ë¯¸ì§€ì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¡œì»¬)"""
    try:
        image = Image.open(io.BytesIO(file_content))
        text = pytesseract.image_to_string(image, lang='kor+eng')
        extracted_text = text.strip() if text.strip() else "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return f"ğŸ–¼ï¸ ì´ë¯¸ì§€ OCR ê²°ê³¼:\n\n{extracted_text}"
    except Exception as e:
        return f"ì´ë¯¸ì§€ OCR ì˜¤ë¥˜: {str(e)}"


async def process_image_with_hybrid_approach(file_content: bytes, filename: str) -> str:
    """ì´ë¯¸ì§€ë¥¼ OCRê³¼ GPT-4o Visionì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬ (í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼)"""
    try:
        print(f"ğŸ–¼ï¸ Processing image with hybrid approach: {filename}")
        
        # 1. GPT-4o Vision ë¶„ì„ ì‹œë„
        vision_result = await analyze_image_with_gpt4o_vision(file_content, filename)
        
        # 2. OCR ë¶„ì„ë„ ìˆ˜í–‰ (í…ìŠ¤íŠ¸ ì¶”ì¶œ ë³´ì™„)
        ocr_result = await extract_text_from_image_local(file_content)
        
        # 3. ê²°ê³¼ ê²°í•©
        combined_result = f"""ğŸ“‹ **ì´ë¯¸ì§€ ì¢…í•© ë¶„ì„ ê²°ê³¼** (íŒŒì¼: {filename})

{vision_result}

---

{ocr_result}

---

ğŸ’¡ **ë¶„ì„ ë°©ë²•**: GPT-4o Vision APIì™€ OCRì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ì‹œê°ì  ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤."""

        return combined_result
        
    except Exception as e:
        print(f"Hybrid image processing error: {e}")
        # í´ë°±: OCRë§Œ ì‚¬ìš©
        return await extract_text_from_image_local(file_content)


def is_image_file(file: any) -> bool:
    """íŒŒì¼ì´ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸"""
    if hasattr(file, 'content_type') and file.content_type:
        return file.content_type.startswith('image/')
    if hasattr(file, 'filename') and file.filename:
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']
        return any(file.filename.lower().endswith(ext) for ext in image_extensions)
    return False


async def create_multimodal_message_content(text: str, image_files: list = None) -> list:
    """ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ì½˜í…ì¸  ìƒì„± (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)"""
    content = []
    
    # í…ìŠ¤íŠ¸ ì¶”ê°€
    if text:
        content.append({"type": "text", "text": text})
    
    # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì¶”ê°€
    if image_files:
        for file in image_files:
            try:
                file_content = await file.read()
                base64_image = encode_image_to_base64(file_content)
                
                # ì´ë¯¸ì§€ íƒ€ì… ê°ì§€
                content_type = file.content_type or 'image/jpeg'
                image_format = content_type.split('/')[-1] if '/' in content_type else 'jpeg'
                
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{content_type};base64,{base64_image}",
                        "detail": "high"
                    }
                })
                
                print(f"ğŸ–¼ï¸ Added image to multimodal message: {file.filename} ({image_format})")
                
            except Exception as e:
                print(f"Failed to process image {file.filename}: {e}")
                # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ì•Œë¦¼ ì¶”ê°€
                content.append({
                    "type": "text", 
                    "text": f"\n[ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {file.filename} - {str(e)}]"
                })
    
    return content


async def send_multimodal_message_to_gpt4o(
    conversation_messages: list,
    user_text: str,
    image_files: list = None,
    model: str = "gpt-4o",
    tools: list = None
) -> str:
    """ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ë¥¼ GPT-4oì— ì „ì†¡"""
    try:
        # ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ìƒì„±
        multimodal_content = await create_multimodal_message_content(user_text, image_files)
        
        # ê¸°ì¡´ ëŒ€í™”ì— ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ì¶”ê°€
        messages = conversation_messages.copy()
        messages.append({
            "role": "user",
            "content": multimodal_content
        })
        
        # GPT-4o API í˜¸ì¶œ ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
        chat_params = {
            "model": model,
            "messages": messages,
            "max_tokens": 2000,
            "temperature": 0.7
        }
        
        # ë„êµ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if tools:
            chat_params["tools"] = tools
            chat_params["tool_choice"] = "auto"
        
        # GPT-4o Vision API í˜¸ì¶œ
        response = await client.chat.completions.create(**chat_params)
        
        # Function calls ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
        if response.choices[0].message.tool_calls:
            content = response.choices[0].message.content or ""
            
            for tool_call in response.choices[0].message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                result = await execute_google_function(function_name, function_args)
                
                if isinstance(result, dict) and "error" in result:
                    content += f"\n\n## âŒ {function_name} ì‹¤í–‰ ì˜¤ë¥˜\n\n{result['error']}"
                else:
                    content += f"\n\n## âœ… {function_name} ì‹¤í–‰ ì™„ë£Œ\n\n"
                    if isinstance(result, (dict, list)):
                        content += f"```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```"
                    else:
                        content += str(result)
            
            return content
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"Multimodal message error: {e}")
        raise e


async def process_uploaded_file(file: UploadFile, session_id: str = None, add_to_vector_store: bool = False) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OpenAI Files API ìš°ì„  ì‚¬ìš©, ë²¡í„° ìŠ¤í† ì–´ í†µí•©)"""
    try:
        file_content = await file.read()
        file_type = file.content_type.lower() if file.content_type else ""
        filename = file.filename or "unknown_file"

        print(f"ğŸ“ Processing uploaded file: {filename} ({file_type})")

        # íŒŒì¼ í¬ê¸° í™•ì¸ (OpenAI ì œí•œ: 512MB)
        file_size_mb = len(file_content) / (1024 * 1024)

        # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¸
        supported_types = [
            "application/pdf",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            "text/plain"
        ]

        supported_extensions = [".pdf", ".docx", ".txt"]
        is_supported_type = (
                file_type in supported_types or
                any(filename.lower().endswith(ext) for ext in supported_extensions) or
                file_type.startswith('image/')
        )

        # OpenAI Files API ì‚¬ìš© ì¡°ê±´
        use_openai_files = (
                is_supported_type and
                file_size_mb < 500 and  # 512MB ì œí•œë³´ë‹¤ ì•½ê°„ ë‚®ê²Œ
                file_type != "text/plain"  # í…ìŠ¤íŠ¸ íŒŒì¼ì€ ë¡œì»¬ì—ì„œ ì²˜ë¦¬
        )

        if use_openai_files:
            print(f"ğŸš€ Using OpenAI Files API for enhanced processing")
            return await process_file_with_openai(file_content, filename, file_type, session_id, add_to_vector_store)
        else:
            print(f"ğŸ”„ Using local processing (file too large or unsupported)")
            return await process_file_locally(file_content, filename, file_type)

    except Exception as e:
        print(f"ğŸš¨ File processing error: {e}")
        return f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def generate_id() -> str:
    return str(uuid.uuid4())


def create_session(title: str = None) -> ChatSession:
    session_id = generate_id()
    session = ChatSession(
        id=session_id,
        title=title or f"ìƒˆ ì±„íŒ… {len(sessions_db) + 1}",
        createdAt=datetime.now(),
        updatedAt=datetime.now(),
        messageCount=0,
        titleGenerated=False,
        titleGeneratedAt=None
    )

    sessions_db[session_id] = session.dict()
    messages_db[session_id] = []
    return session


def get_session(session_id: str) -> ChatSession:
    if session_id not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session_data = sessions_db[session_id].copy()
    
    # Handle legacy sessions missing required fields
    if "title" not in session_data:
        session_data["title"] = f"ì±„íŒ… {session_id[:8]}"
    if "updatedAt" not in session_data:
        session_data["updatedAt"] = session_data.get("createdAt", datetime.now())
    if "createdAt" not in session_data:
        session_data["createdAt"] = datetime.now()
    if "messageCount" not in session_data:
        session_data["messageCount"] = len(messages_db.get(session_id, []))
    if "titleGenerated" not in session_data:
        session_data["titleGenerated"] = False
    if "titleGeneratedAt" not in session_data:
        session_data["titleGeneratedAt"] = None
    
    # Update the stored session with missing fields for future use
    sessions_db[session_id].update(session_data)
    
    return ChatSession(**session_data)


def migrate_legacy_sessions():
    """Legacy sessionsì„ ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    migrated_count = 0
    for session_id, session_data in sessions_db.items():
        updated = False
        if "title" not in session_data:
            session_data["title"] = f"ì±„íŒ… {session_id[:8]}"
            updated = True
        if "updatedAt" not in session_data:
            session_data["updatedAt"] = session_data.get("createdAt", datetime.now())
            updated = True
        if "createdAt" not in session_data:
            session_data["createdAt"] = datetime.now()
            updated = True
        if "messageCount" not in session_data:
            session_data["messageCount"] = len(messages_db.get(session_id, []))
            updated = True
        if "titleGenerated" not in session_data:
            session_data["titleGenerated"] = False
            updated = True
        if "titleGeneratedAt" not in session_data:
            session_data["titleGeneratedAt"] = None
            updated = True
        
        if updated:
            migrated_count += 1
    
    if migrated_count > 0:
        logger.info(f"âœ… Migrated {migrated_count} legacy sessions to new schema")


def update_session_message_count(session_id: str):
    if session_id in sessions_db:
        sessions_db[session_id]["messageCount"] = len(messages_db.get(session_id, []))
        sessions_db[session_id]["updatedAt"] = datetime.now()


def update_session_title(session_id: str, new_title: str, auto_generated: bool = False):
    """ì„¸ì…˜ ì œëª© ì—…ë°ì´íŠ¸"""
    if session_id in sessions_db:
        sessions_db[session_id]["title"] = new_title
        sessions_db[session_id]["updatedAt"] = datetime.now()
        if auto_generated:
            sessions_db[session_id]["titleGenerated"] = True
            sessions_db[session_id]["titleGeneratedAt"] = datetime.now()


async def auto_generate_title_if_needed(session_id: str) -> Optional[str]:
    """ìë™ ì œëª© ìƒì„±ì´ í•„ìš”í•œ ê²½ìš° ìƒì„±í•˜ì—¬ ì—…ë°ì´íŠ¸"""
    if not title_generator or not TITLE_GENERATOR_AVAILABLE:
        return None
    
    try:
        # ì„¸ì…˜ê³¼ ë©”ì‹œì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        if session_id not in sessions_db or session_id not in messages_db:
            return None
            
        session_data = sessions_db[session_id]
        messages = messages_db[session_id]
        
        # ì œëª© ìƒì„± ì¡°ê±´ í™•ì¸
        if not title_generator.should_generate_title(messages, session_data["title"]):
            return None
        
        # ì œëª© ìƒì„±
        new_title = await title_generator.generate_title(messages)
        
        if new_title:
            # ì œëª© ì—…ë°ì´íŠ¸
            update_session_title(session_id, new_title, auto_generated=True)
            logger.info(f"Auto-generated title for session {session_id}: {new_title}")
            return new_title
        else:
            # í´ë°± ì œëª© ì‚¬ìš©
            fallback_title = title_generator.get_fallback_title(messages)
            update_session_title(session_id, fallback_title, auto_generated=True)
            logger.info(f"Using fallback title for session {session_id}: {fallback_title}")
            return fallback_title
            
    except Exception as e:
        logger.error(f"Failed to generate title for session {session_id}: {str(e)}")
        return None


# ì´ˆê¸° ë°ëª¨ ë°ì´í„° ìƒì„±
def initialize_demo_data():
    if not sessions_db:
        # ë°ëª¨ ì„¸ì…˜ 1
        demo_session_1 = create_session("ì˜ì—… ë°ì´í„° ë¶„ì„")
        messages_db[demo_session_1.id] = [
            {
                "id": generate_id(),
                "content": "ì´ë²ˆ ë¶„ê¸° ì˜ì—… ì„±ê³¼ëŠ” ì–´ë–¤ê°€ìš”?",
                "role": "user",
                "timestamp": datetime.now(),
                "sessionId": demo_session_1.id
            },
            {
                "id": generate_id(),
                "content": "ì´ë²ˆ ë¶„ê¸° ì˜ì—… ì„±ê³¼ë¥¼ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ëª©í‘œ ëŒ€ë¹„ 115% ë‹¬ì„±í–ˆìœ¼ë©°, íŠ¹íˆ ìƒˆë¡œìš´ ê³ ê° í™•ë³´ ë¶€ë¶„ì—ì„œ ì¢‹ì€ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.",
                "role": "assistant",
                "timestamp": datetime.now(),
                "sessionId": demo_session_1.id
            }
        ]

        # ë°ëª¨ ì„¸ì…˜ 2
        demo_session_2 = create_session("í”„ë¡œì íŠ¸ í˜„í™© ì¡°íšŒ")
        messages_db[demo_session_2.id] = [
            {
                "id": generate_id(),
                "content": "ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ë³´ì—¬ì£¼ì„¸ìš”",
                "role": "user",
                "timestamp": datetime.now(),
                "sessionId": demo_session_2.id
            },
            {
                "id": generate_id(),
                "content": "í˜„ì¬ ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ëŠ” ì´ 12ê°œì…ë‹ˆë‹¤. ê·¸ ì¤‘ 5ê°œëŠ” ê°œë°œ ë‹¨ê³„, 4ê°œëŠ” í…ŒìŠ¤íŠ¸ ë‹¨ê³„, 3ê°œëŠ” ë°°í¬ ì¤€ë¹„ ë‹¨ê³„ì— ìˆìŠµë‹ˆë‹¤.",
                "role": "assistant",
                "timestamp": datetime.now(),
                "sessionId": demo_session_2.id
            }
        ]

        # ë°ëª¨ ì„¸ì…˜ 3
        demo_session_3 = create_session("ê³ ê°ì‚¬ ì •ë³´ ë¬¸ì˜")
        messages_db[demo_session_3.id] = []

        # ë©”ì‹œì§€ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        for session_id in sessions_db.keys():
            update_session_message_count(session_id)


# ì•± ì‹œì‘ ì‹œ ë°ëª¨ ë°ì´í„° ì´ˆê¸°í™” (ë¹„í™œì„±í™”)
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    # initialize_demo_data()  # ë°ëª¨ ë°ì´í„° ìƒì„± ë¹„í™œì„±í™”
    migrate_legacy_sessions()  # Legacy sessions ë§ˆì´ê·¸ë ˆì´ì…˜
    yield
    # Shutdown
    pass


app = FastAPI(
    title="NSales Pro Chat API",
    description="AI ì±„íŒ… ì„œë¹„ìŠ¤ API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:5174", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# API ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/")
async def root():
    return {"message": "NSales Pro Chat API", "version": "1.0.0"}


@app.get("/api/v1/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now()}


@app.get("/api/v1/models")
async def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return {"models": AVAILABLE_MODELS}


# Google ì„œë¹„ìŠ¤ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/v1/google/auth")
async def google_auth():
    """Google OAuth2 ì¸ì¦ ì‹œì‘"""
    if not GOOGLE_SERVICES_AVAILABLE:
        raise HTTPException(status_code=503, detail="Google ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    auth_url = auth_service.get_authorization_url()
    if not auth_url:
        raise HTTPException(status_code=500, detail="ì¸ì¦ URL ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    return {"auth_url": auth_url}


@app.get("/api/v1/google/callback")
async def google_callback(code: str):
    """Google OAuth2 ì½œë°± ì²˜ë¦¬"""
    if not GOOGLE_SERVICES_AVAILABLE:
        raise HTTPException(status_code=503, detail="Google ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    success = auth_service.handle_callback(code)
    if success:
        print("âœ… Google OAuth ì¸ì¦ ì„±ê³µ! ì‚¬ìš©ìê°€ 5173 í¬íŠ¸ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë©ë‹ˆë‹¤.")
        return RedirectResponse(url="http://localhost:5173?google_auth=success&message=Google ì„œë¹„ìŠ¤ ì—°ë™ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ Google OAuth ì¸ì¦ ì‹¤íŒ¨! ì‚¬ìš©ìê°€ 5173 í¬íŠ¸ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë©ë‹ˆë‹¤.")
        return RedirectResponse(url="http://localhost:5173?google_auth=error&message=Google ì„œë¹„ìŠ¤ ì—°ë™ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


@app.get("/api/v1/google/status")
async def google_status():
    """Google ì¸ì¦ ìƒíƒœ í™•ì¸"""
    if not GOOGLE_SERVICES_AVAILABLE:
        return {"authenticated": False, "error": "Google ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    return {
        "authenticated": auth_service.is_authenticated(),
        "services_available": True
    }


# ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬
@app.post("/api/v1/chat/sessions", response_model=ChatSession)
async def create_chat_session(request: SessionCreateRequest):
    session = create_session(request.title)
    return session


@app.get("/api/v1/chat/sessions", response_model=ChatSessionList)
async def get_chat_sessions(search: ChatSearch = Depends()):
    sessions = []
    for session_id, session_data in sessions_db.items():
        # Handle legacy sessions missing required fields
        cleaned_session = session_data.copy()
        if "title" not in cleaned_session:
            cleaned_session["title"] = f"ì±„íŒ… {session_id[:8]}"
        if "updatedAt" not in cleaned_session:
            cleaned_session["updatedAt"] = cleaned_session.get("createdAt", datetime.now())
        if "createdAt" not in cleaned_session:
            cleaned_session["createdAt"] = datetime.now()
        if "messageCount" not in cleaned_session:
            cleaned_session["messageCount"] = len(messages_db.get(session_id, []))
        if "titleGenerated" not in cleaned_session:
            cleaned_session["titleGenerated"] = False
        if "titleGeneratedAt" not in cleaned_session:
            cleaned_session["titleGeneratedAt"] = None
        
        sessions.append(cleaned_session)
        # Update the stored session with missing fields for future use
        sessions_db[session_id].update(cleaned_session)

    # ê²€ìƒ‰ í•„í„° ì ìš©
    if search.query:
        sessions = [
            s for s in sessions
            if search.query.lower() in s["title"].lower()
        ]

    # ì •ë ¬ (ìµœì‹ ìˆœ)
    sessions.sort(key=lambda x: x["updatedAt"], reverse=True)

    # í˜ì´ì§•
    total = len(sessions)
    start = search.page * search.size
    end = start + search.size
    paged_sessions = sessions[start:end]

    return ChatSessionList(
        sessions=[ChatSession(**s) for s in paged_sessions],
        totalElements=total,
        totalPages=(total + search.size - 1) // search.size,
        currentPage=search.page,
        size=search.size
    )


@app.get("/api/v1/chat/sessions/{session_id}", response_model=ChatSession)
async def get_chat_session(session_id: str):
    return get_session(session_id)


@app.patch("/api/v1/chat/sessions/{session_id}")
async def update_chat_session(session_id: str, request: SessionUpdateRequest):
    if session_id not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")

    sessions_db[session_id]["title"] = request.title
    sessions_db[session_id]["updatedAt"] = datetime.now()
    return {"message": "Session updated successfully"}


@app.delete("/api/v1/chat/sessions/{session_id}")
async def delete_chat_session(session_id: str):
    if session_id not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")

    del sessions_db[session_id]
    if session_id in messages_db:
        del messages_db[session_id]

    return {"message": "Session deleted successfully"}


@app.post("/api/v1/chat/sessions/{session_id}/generate-title")
async def generate_session_title(session_id: str):
    """ì„¸ì…˜ì˜ AI ê¸°ë°˜ ì œëª© ìƒì„± (ìˆ˜ë™ íŠ¸ë¦¬ê±°)"""
    if session_id not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")
    
    try:
        # ì„¸ì…˜ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        messages = messages_db.get(session_id, [])
        
        if len(messages) < 2:
            return {"success": False, "message": "ë©”ì‹œì§€ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
        
        # ì œëª© ìƒì„±
        generated_title = await title_generator.generate_title(messages)
        
        if not generated_title:
            # ìƒì„± ì‹¤íŒ¨ì‹œ í´ë°± ì œëª© ì‚¬ìš©
            fallback_title = title_generator.get_fallback_title(messages)
            return {"success": False, "title": fallback_title, "message": "AI ì œëª© ìƒì„± ì‹¤íŒ¨, í´ë°± ì œëª© ì‚¬ìš©"}
        
        # ì„¸ì…˜ ì œëª© ì—…ë°ì´íŠ¸
        sessions_db[session_id]["title"] = generated_title
        sessions_db[session_id]["titleGenerated"] = True
        sessions_db[session_id]["titleGeneratedAt"] = datetime.now()
        sessions_db[session_id]["updatedAt"] = datetime.now()
        
        logger.info(f"âœ¨ Manual title generated for session {session_id}: {generated_title}")
        
        return {
            "success": True, 
            "title": generated_title,
            "message": "AI ì œëª©ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"âŒ Manual title generation failed for session {session_id}: {str(e)}")
        return {"success": False, "message": f"ì œëª© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}


# ë©”ì‹œì§€ ê´€ë¦¬
@app.get("/api/v1/chat/sessions/{session_id}/messages", response_model=ChatHistory)
async def get_message_history(session_id: str):
    if session_id not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")

    messages = messages_db.get(session_id, [])
    return ChatHistory(
        messages=[ChatMessage(**msg) for msg in messages],
        sessionId=session_id,
        totalCount=len(messages)
    )


@app.get("/api/v1/chat/sessions/{session_id}/tokens")
async def get_token_usage(session_id: str):
    """ì„¸ì…˜ì˜ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¡°íšŒ"""
    if session_id not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")

    # í˜„ì¬ í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
    current_messages = messages_db.get(session_id, [])
    current_tokens = calculate_conversation_tokens(current_messages)

    # ì €ì¥ëœ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
    usage_info = token_usage_db.get(session_id, {
        "total_tokens": current_tokens,
        "messages_count": len(current_messages),
        "optimizations": 0,
        "tokens_saved": 0,
        "last_updated": datetime.now().isoformat()
    })

    # ì‹¤ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
    usage_info.update({
        "current_tokens": current_tokens,
        "max_tokens": MAX_CONVERSATION_TOKENS,
        "optimization_threshold": SUMMARY_TRIGGER_TOKENS,
        "has_summary": session_id in conversation_summaries,
        "efficiency_percentage": round((1 - current_tokens / MAX_CONVERSATION_TOKENS) * 100,
                                       1) if current_tokens < MAX_CONVERSATION_TOKENS else 0
    })

    return usage_info


@app.post("/api/v1/chat/messages/with-files")
async def send_message_with_files(
        content: str = Form(...),
        sessionId: str = Form(...),
        files: List[UploadFile] = File(default=[]),
        model: str = Form(default="gpt-4o")
):
    """íŒŒì¼ ì²¨ë¶€ë¥¼ ì§€ì›í•˜ëŠ” ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡"""
    try:
        # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
        if sessionId not in sessions_db:
            raise HTTPException(status_code=404, detail="Session not found")

        session_messages = messages_db.get(sessionId, [])

        # íŒŒì¼ ë¶„ë¥˜: ì´ë¯¸ì§€ íŒŒì¼ê³¼ ë¬¸ì„œ íŒŒì¼ ë¶„ë¦¬
        image_files = []
        document_files = []
        file_contents = []
        
        if files:
            for file in files:
                if file.filename:  # íŒŒì¼ì´ ì‹¤ì œë¡œ ì—…ë¡œë“œëœ ê²½ìš°
                    print(f"Processing file: {file.filename}, type: {file.content_type}")
                    
                    if is_image_file(file):
                        # ì´ë¯¸ì§€ íŒŒì¼ì€ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë³„ë„ ë³´ê´€
                        image_files.append(file)
                        print(f"ğŸ–¼ï¸ Image file detected: {file.filename}")
                    else:
                        # ë¬¸ì„œ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                        document_files.append(file)
                        file_text = await process_uploaded_file(file, sessionId, add_to_vector_store=True)
                        file_contents.append(f"[íŒŒì¼: {file.filename}]\n{file_text}")

        # ë©”ì‹œì§€ ë‚´ìš© êµ¬ì„± (í…ìŠ¤íŠ¸ + ë¬¸ì„œ íŒŒì¼ ë‚´ìš©)
        message_content = content
        if file_contents:
            message_content += "\n\n" + "\n\n".join(file_contents)
        
        # ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_multimodal = len(image_files) > 0 and model == "gpt-4o"

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        # ë©€í‹°ëª¨ë‹¬ì˜ ê²½ìš° ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€ í‘œì‹œ
        display_content = message_content
        if use_multimodal and image_files:
            image_info = ", ".join([f"ğŸ–¼ï¸ {file.filename}" for file in image_files])
            display_content += f"\n\n[ì²¨ë¶€ëœ ì´ë¯¸ì§€: {image_info}]"
            
        user_message = ChatMessage(
            id=generate_id(),
            content=display_content,
            role="user",
            timestamp=datetime.now(),
            sessionId=sessionId
        )
        session_messages.append(user_message.model_dump())

        # OpenAI APIì— ì „ë‹¬í•  ë©”ì‹œì§€ êµ¬ì„±
        system_prompt = "ë‹¹ì‹ ì€ NSales Proì˜ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì˜ì—… ë°ì´í„° ë¶„ì„, í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ, ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ë„ì›€ì„ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì²¨ë¶€ëœ íŒŒì¼ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. ìµœì‹  ì •ë³´ê°€ í•„ìš”í•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°, ë‰´ìŠ¤, ì‹œì¥ ë™í–¥ ë“±ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ì›¹ ê²€ìƒ‰ì„ ì ê·¹ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìµœì‹ ì˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”."
        
        conversation_messages = [{"role": "system", "content": system_prompt}]

        # ê¸°ì¡´ ëŒ€í™” ë‚´ìš© ì¶”ê°€ (ìµœê·¼ 20ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€)
        recent_messages = session_messages[-21:] if len(session_messages) > 21 else session_messages[:-1]  # í˜„ì¬ ë©”ì‹œì§€ ì œì™¸
        for msg in recent_messages:
            conversation_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })

        # ì„ íƒëœ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        selected_model = model if model in AVAILABLE_MODELS else "gpt-4o"
        model_config = AVAILABLE_MODELS[selected_model]

        # OpenAI API í˜¸ì¶œ
        try:
            print(f"Using model: {selected_model} ({model_config['name']})")
            print(f"Conversation length: {len(conversation_messages)} messages")
            print(f"Files processed: {len(file_contents)} documents, {len(image_files)} images")
            print(f"Multimodal mode: {use_multimodal}")

            # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ êµ¬ì„±
            available_tools = []
            if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
                available_tools.extend(get_google_tools())

            if use_multimodal:
                # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
                print("ğŸ”„ Using multimodal message processing...")
                ai_content = await send_multimodal_message_to_gpt4o(
                    conversation_messages,
                    message_content,
                    image_files,
                    selected_model,
                    available_tools
                )
            else:
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                print("ğŸ”„ Using traditional text-only processing...")
                
                # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (í…ìŠ¤íŠ¸ë§Œ)
                conversation_messages.append({"role": "user", "content": message_content})
                
                # ì›¹ ê²€ìƒ‰ ì—¬ë¶€ëŠ” form ë°ì´í„°ì—ì„œ í™•ì¸ (ì¼ë‹¨ Falseë¡œ ì„¤ì •)
                needs_web_search = False  # íŒŒì¼ ì—…ë¡œë“œ ì‹œì—ëŠ” ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”

                # ìµœì ì˜ OpenAI API ì„ íƒí•˜ì—¬ ì‚¬ìš©
                ai_content = await create_response_with_best_api(
                    sessionId,
                    selected_model,
                    system_prompt,
                    message_content,  # íŒŒì¼ ë‚´ìš©ì´ í¬í•¨ëœ ë©”ì‹œì§€
                    conversation_messages,
                    available_tools,
                    needs_web_search,
                    model_config
                )

            print(f"OpenAI Response: {ai_content}")

        except Exception as e:
            # ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬
            ai_content = handle_openai_error(e, user_content=content)

        # AI ì‘ë‹µ ì €ì¥
        ai_message = ChatMessage(
            id=generate_id(),
            content=ai_content,
            role="assistant",
            timestamp=datetime.now(),
            sessionId=sessionId
        )
        session_messages.append(ai_message.model_dump())

        # ë©”ì‹œì§€ ì €ì¥
        messages_db[sessionId] = session_messages

        # ì„¸ì…˜ ì—…ë°ì´íŠ¸
        sessions_db[sessionId]["messageCount"] = len(session_messages)
        sessions_db[sessionId]["updatedAt"] = datetime.now()

        return {
            "userMessage": user_message,
            "aiMessage": ai_message,
            "success": True
        }

    except Exception as e:
        print(f"Error in send_message_with_files: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/chat/messages", response_model=ChatResponse)
async def send_message(request: ChatRequest):
    if request.sessionId not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    user_message = ChatMessage(
        id=generate_id(),
        content=request.content,
        role="user",
        timestamp=datetime.now(),
        sessionId=request.sessionId
    )

    if request.sessionId not in messages_db:
        messages_db[request.sessionId] = []

    messages_db[request.sessionId].append(user_message.dict())

    # ì„¸ì…˜ì˜ ê¸°ì¡´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    session_messages = messages_db.get(request.sessionId, [])

    # Google ì„œë¹„ìŠ¤ ì‚¬ìš© ì•ˆë‚´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = "ë‹¹ì‹ ì€ NSales Proì˜ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì˜ì—… ë°ì´í„° ë¶„ì„, í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ, ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ë„ì›€ì„ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë¬¸ë§¥ì„ ìœ ì§€í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ìµœì‹  ì •ë³´ê°€ í•„ìš”í•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°, ë‰´ìŠ¤, ì‹œì¥ ë™í–¥ ë“±ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ì›¹ ê²€ìƒ‰ì„ ì ê·¹ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìµœì‹ ì˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”."

    # ë©˜ì…˜ ê¸°ë°˜ ì„œë¹„ìŠ¤ í™œì„±í™” ë¡œì§
    mention_detected = False
    google_mention_keywords = ['@ìº˜ë¦°ë”', '@ë©”ì¼', '@ì¼ì •ìƒì„±', '@ë¹ˆì‹œê°„']

    for keyword in google_mention_keywords:
        if keyword in request.content:
            mention_detected = True
            break

    # Google ì„œë¹„ìŠ¤ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë©˜ì…˜ì´ ê°ì§€ëœ ê²½ìš° ì•ˆë‚´ ì¶”ê°€
    if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated() and mention_detected:
        system_prompt += "\n\n**ğŸ¯ Google ì„œë¹„ìŠ¤ ë©˜ì…˜ ê°ì§€ë¨:**\nì‚¬ìš©ìê°€ @ë©˜ì…˜ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ë°˜ë“œì‹œ í˜¸ì¶œí•˜ì—¬ ìš”ì²­ì„ ì²˜ë¦¬í•˜ì„¸ìš”:\n- @ìº˜ë¦°ë” â†’ get_calendar_events í•¨ìˆ˜ í˜¸ì¶œ\n- @ë©”ì¼ â†’ get_emails ë˜ëŠ” send_email í•¨ìˆ˜ í˜¸ì¶œ\n- @ì¼ì •ìƒì„± â†’ create_calendar_event í•¨ìˆ˜ í˜¸ì¶œ\n- @ë¹ˆì‹œê°„ â†’ find_free_time í•¨ìˆ˜ í˜¸ì¶œ\n\në©˜ì…˜ì´ í¬í•¨ëœ ìš”ì²­ì€ ë°˜ë“œì‹œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."
    elif GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
        system_prompt += "\n\n**Google ì„œë¹„ìŠ¤ ì—°ë™ ì•ˆë‚´:**\nì‚¬ìš©ìê°€ ìº˜ë¦°ë”, ì¼ì •, ìŠ¤ì¼€ì¤„, Gmail, ì´ë©”ì¼ ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë©´ ë‹¤ìŒ í•¨ìˆ˜ë“¤ì„ ì ê·¹ í™œìš©í•˜ì„¸ìš”:\n- get_calendar_events: ìº˜ë¦°ë” ì¼ì • ì¡°íšŒ (ì˜¤ëŠ˜, ì´ë²ˆì£¼, ì´ë²ˆë‹¬ ë“±)\n- create_calendar_event: ìƒˆ ì¼ì • ìƒì„±\n- send_email: ì´ë©”ì¼ ì „ì†¡\n- get_emails: ì´ë©”ì¼ ì¡°íšŒ\n- find_free_time: ë¹ˆ ì‹œê°„ ì°¾ê¸°\n\nì‚¬ìš©ìê°€ 'ìº˜ë¦°ë”', 'ì¼ì •', 'ìŠ¤ì¼€ì¤„' ë“±ì˜ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë°˜ë“œì‹œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œê³µí•˜ì„¸ìš”."

    # ğŸ“Š í† í° ìµœì í™”ëœ ëŒ€í™” ë©”ì‹œì§€ êµ¬ì„±
    optimized_messages = await get_optimized_conversation_messages(request.sessionId, max_messages=18)

    conversation_messages = [{"role": "system", "content": system_prompt}]

    # ìµœì í™”ëœ ë©”ì‹œì§€ ì¶”ê°€
    for msg in optimized_messages:
        if msg.get("role") != "system":  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì´ë¯¸ ì¶”ê°€ë¨
            conversation_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })

    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    conversation_messages.append({"role": "user", "content": request.content})

    # ì„ íƒëœ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    selected_model = request.model if request.model in AVAILABLE_MODELS else "gpt-4o"
    model_config = AVAILABLE_MODELS[selected_model]

    # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ êµ¬ì„±
    available_tools = []
    if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
        available_tools.extend(get_google_tools())
        print(f"ğŸ› ï¸ Google ë„êµ¬ {len(get_google_tools())}ê°œ ì¶”ê°€ë¨")

    # ê°œì„ ëœ OpenAI API í˜¸ì¶œ (Responses API ìš°ì„  ì‚¬ìš©)
    try:
        print(f"Using model: {selected_model} ({model_config['name']})")
        print(f"Conversation length: {len(conversation_messages)} messages")

        # ì›¹ ê²€ìƒ‰ ì—¬ë¶€ í™•ì¸
        needs_web_search = getattr(request, 'webSearch', False)

        # ìµœì ì˜ OpenAI API ì„ íƒí•˜ì—¬ ì‚¬ìš©
        ai_content = await create_response_with_best_api(
            request.sessionId,
            selected_model,
            system_prompt,
            request.content,
            conversation_messages,
            available_tools,
            needs_web_search,
            model_config
        )

        print(f"OpenAI Response: {ai_content}")

    except Exception as e:
        # ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬
        ai_content = handle_openai_error(e, user_content=request.content)

    # AI ì‘ë‹µ ì €ì¥
    ai_message = ChatMessage(
        id=generate_id(),
        content=ai_content,
        role="assistant",
        timestamp=datetime.now(),
        sessionId=request.sessionId
    )

    messages_db[request.sessionId].append(ai_message.dict())
    update_session_message_count(request.sessionId)

    # ìë™ ì œëª© ìƒì„± ì‹œë„ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
    await auto_generate_title_if_needed(request.sessionId)

    return ChatResponse(**ai_message.dict())


# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (í†µí•© API ì„ íƒ ë¡œì§ ì‚¬ìš©)
@app.post("/api/v1/chat/stream")
async def stream_chat(request: ChatRequest):
    """í†µí•© API ì„ íƒì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"""

    # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
    if request.sessionId not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    user_message = ChatMessage(
        id=generate_id(),
        content=request.content,
        role="user",
        timestamp=datetime.now(),
        sessionId=request.sessionId
    )

    if request.sessionId not in messages_db:
        messages_db[request.sessionId] = []

    messages_db[request.sessionId].append(user_message.dict())

    # ëª¨ë¸ ì„ íƒ ë° ì„¤ì •
    selected_model = request.model if request.model in AVAILABLE_MODELS else "gpt-4o"
    model_config = AVAILABLE_MODELS[selected_model]

    # ì„¸ì…˜ì˜ ê¸°ì¡´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    session_messages = messages_db.get(request.sessionId, [])

    # í˜„ì¬ í•œêµ­ ì‹œê°„ ì •ë³´ ìƒì„±
    korea_tz = timezone(timedelta(hours=9))
    current_time_kst = datetime.now(korea_tz)

    # Google ì„œë¹„ìŠ¤ ì‚¬ìš© ì•ˆë‚´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = f"""ë‹¹ì‹ ì€ NSales Proì˜ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì˜ì—… ë°ì´í„° ë¶„ì„, í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ, ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ë„ì›€ì„ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë¬¸ë§¥ì„ ìœ ì§€í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ìµœì‹  ì •ë³´ê°€ í•„ìš”í•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°, ë‰´ìŠ¤, ì‹œì¥ ë™í–¥ ë“±ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ì›¹ ê²€ìƒ‰ì„ ì ê·¹ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìµœì‹ ì˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

**í˜„ì¬ ì‹œê°„ ì •ë³´:**
- í˜„ì¬ ë‚ ì§œ: {current_time_kst.strftime('%Yë…„ %mì›” %dì¼ (%A)')}
- í˜„ì¬ ì‹œê°„: {current_time_kst.strftime('%Hì‹œ %Më¶„')}
- ì‹œê°„ëŒ€: í•œêµ­ í‘œì¤€ì‹œ (KST, UTC+9)

"ì˜¤ëŠ˜", "ì´ë²ˆ ì£¼", "ì´ë²ˆ ë‹¬" ë“±ì˜ ì‹œê°„ í‘œí˜„ì„ ì‚¬ìš©í•  ë•ŒëŠ” ìœ„ì˜ í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ í•´ì„í•´ì£¼ì„¸ìš”."""

    # ë©˜ì…˜ ê¸°ë°˜ ì„œë¹„ìŠ¤ í™œì„±í™” ë¡œì§
    mention_detected = False
    google_mention_keywords = ['@ìº˜ë¦°ë”', '@ë©”ì¼', '@ì¼ì •ìƒì„±', '@ë¹ˆì‹œê°„']

    for keyword in google_mention_keywords:
        if keyword in request.content:
            mention_detected = True
            break

    # Google ì„œë¹„ìŠ¤ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë©˜ì…˜ì´ ê°ì§€ëœ ê²½ìš° ì•ˆë‚´ ì¶”ê°€
    if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated() and mention_detected:
        system_prompt += "\n\n**ğŸ¯ Google ì„œë¹„ìŠ¤ ë©˜ì…˜ ê°ì§€ë¨:**\nì‚¬ìš©ìê°€ @ë©˜ì…˜ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ë°˜ë“œì‹œ í˜¸ì¶œí•˜ì—¬ ìš”ì²­ì„ ì²˜ë¦¬í•˜ì„¸ìš”:\n- @ìº˜ë¦°ë” â†’ get_calendar_events í•¨ìˆ˜ í˜¸ì¶œ\n- @ë©”ì¼ â†’ get_emails ë˜ëŠ” send_email í•¨ìˆ˜ í˜¸ì¶œ\n- @ì¼ì •ìƒì„± â†’ create_calendar_event í•¨ìˆ˜ í˜¸ì¶œ\n- @ë¹ˆì‹œê°„ â†’ find_free_time í•¨ìˆ˜ í˜¸ì¶œ\n\në©˜ì…˜ì´ í¬í•¨ëœ ìš”ì²­ì€ ë°˜ë“œì‹œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."
    elif GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
        system_prompt += "\n\n**ğŸ› ï¸ Google ì„œë¹„ìŠ¤ í™œìš© ê°€ëŠ¥:**\nìº˜ë¦°ë” ì¡°íšŒ, ì´ë©”ì¼ ê´€ë¦¬, ì¼ì • ìƒì„± ë“±ì˜ ìš”ì²­ ì‹œ Google í•¨ìˆ˜ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."

    # ğŸ“Š í† í° ìµœì í™”ëœ ëŒ€í™” ë©”ì‹œì§€ êµ¬ì„± (ìŠ¤íŠ¸ë¦¬ë°)
    optimized_messages = await get_optimized_conversation_messages(request.sessionId, max_messages=18)

    conversation_messages = [{"role": "system", "content": system_prompt}]

    # ìµœì í™”ëœ ë©”ì‹œì§€ ì¶”ê°€ (í˜„ì¬ ë©”ì‹œì§€ëŠ” ì œì™¸)
    for msg in optimized_messages:
        if msg.get("role") != "system":  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì´ë¯¸ ì¶”ê°€ë¨
            conversation_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })

    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    conversation_messages.append({
        "role": "user",
        "content": request.content
    })

    # Google ë„êµ¬ ì¤€ë¹„
    available_tools = []
    if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
        available_tools.extend(GOOGLE_TOOLS)
        print(f"ğŸ› ï¸ Google ë„êµ¬ {len(GOOGLE_TOOLS)}ê°œ ì¶”ê°€ë¨")
        print(f"ğŸ¯ ë©˜ì…˜ ê°ì§€: {mention_detected}")
        if mention_detected:
            print(f"ğŸ”¥ ê°•ì œ Function Calling í™œì„±í™” ì˜ˆì •: @ìº˜ë¦°ë” â†’ get_calendar_events")

    # ì›¹ ê²€ìƒ‰ ì—¬ë¶€ í™•ì¸
    needs_web_search = getattr(request, 'webSearch', False)

    # Google ë©˜ì…˜ì´ ê°ì§€ëœ ê²½ìš° ì§ì ‘ Function Calling ì²˜ë¦¬
    if mention_detected and available_tools:
        print("ğŸ¯ ë©˜ì…˜ ê°ì§€ë¨ - ì§ì ‘ Function Calling ì²˜ë¦¬")
        return await stream_with_direct_function_calling(
            request.sessionId,
            selected_model,
            conversation_messages,
            available_tools,
            model_config,
            user_message
        )

    # ì¼ë°˜ì ì¸ ê²½ìš° í†µí•© API ì‚¬ìš©
    return await stream_with_unified_api(
        request.sessionId,
        selected_model,
        system_prompt,
        request.content,
        conversation_messages,
        available_tools,
        needs_web_search,
        model_config,
        user_message,
        mention_detected
    )


async def stream_with_unified_api(
        session_id: str,
        model: str,
        instructions: str,
        user_input: str,
        conversation_messages: List[Dict],
        available_tools: List[Dict],
        needs_web_search: bool,
        model_config: Dict,
        user_message: ChatMessage,
        mention_detected: bool = False
):
    """í†µí•© API ì„ íƒì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""

    async def generate_unified_stream():
        ai_message_id = generate_id()
        full_content = ""

        try:
            # 1. í†µí•© API ì„ íƒ ë¡œì§ ì‹¤í–‰
            print(f"ğŸ” Stream API Selection Debug:")
            print(f"  - Model: {model}")
            print(f"  - supports_assistant: {model_config.get('supports_assistant', False)}")
            print(f"  - needs_web_search: {needs_web_search}")
            print(f"  - available_tools: {len(available_tools) if available_tools else 0}")

            # Assistant APIëŠ” í˜„ì¬ ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ì´ë¯€ë¡œ ê°€ëŠ¥í•œ API ì‚¬ìš©
            # ë¨¼ì € Chat Completionsë¡œ ì‹œë„ (ë” ì•ˆì •ì )
            use_responses_api = needs_web_search and model_config.get("supports_web_search", False)

            if use_responses_api:
                print("ğŸŒ Using Responses API for streaming")
                # Responses API ìŠ¤íŠ¸ë¦¬ë°
                async for chunk_str in stream_with_responses_api(
                        model, instructions, user_input, conversation_messages,
                        available_tools, needs_web_search, model_config, ai_message_id, session_id
                ):
                    # chunk_strì€ ì´ë¯¸ "data: {...}\n\n" í˜•íƒœ
                    if chunk_str.startswith("data:"):
                        try:
                            chunk_data = json.loads(chunk_str[5:].strip())
                            full_content += chunk_data.get("content", "")
                        except:
                            pass
                    yield chunk_str
            else:
                print("ğŸ’¬ Using Chat Completions API for streaming")
                # Chat Completions ìŠ¤íŠ¸ë¦¬ë° (í´ë°±)
                async for chunk_str in stream_with_chat_completions_fallback(
                        model, conversation_messages, ai_message_id, session_id
                ):
                    # chunk_strì€ ì´ë¯¸ "data: {...}\n\n" í˜•íƒœ
                    if chunk_str.startswith("data:"):
                        try:
                            chunk_data = json.loads(chunk_str[5:].strip())
                            full_content += chunk_data.get("content", "")
                        except:
                            pass
                    yield chunk_str

        except Exception as e:
            print(f"ğŸš¨ Streaming error: {e}")
            print(f"ğŸš¨ Error type: {type(e)}")
            print(f"ğŸš¨ Error details: {str(e)}")
            import traceback
            print(f"ğŸš¨ Traceback: {traceback.format_exc()}")
            error_content = handle_openai_error(e, user_content=user_input)

            error_chunk = {
                "id": ai_message_id,
                "content": error_content,
                "role": "assistant",
                "timestamp": datetime.now().isoformat(),
                "sessionId": session_id,
                "isComplete": True
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
            full_content = error_content

        # AI ì‘ë‹µ ì €ì¥
        ai_message = ChatMessage(
            id=ai_message_id,
            content=full_content,
            role="assistant",
            timestamp=datetime.now(),
            sessionId=session_id
        )

        messages_db[session_id].append(ai_message.dict())
        update_session_message_count(session_id)

    return StreamingResponse(
        generate_unified_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )


async def stream_with_responses_api(
        model: str,
        instructions: str,
        user_input: str,
        conversation_messages: List[Dict],
        available_tools: List[Dict],
        needs_web_search: bool,
        model_config: Dict,
        ai_message_id: str,
        session_id: str
):
    """Responses APIë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë°"""

    # Responses APIëŠ” ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ì´ë¯€ë¡œ ì¼ë°˜ ì‘ë‹µ í›„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì „ì†¡
    try:
        print(f"ğŸ” Calling Responses API fallback with:")
        print(f"  - Model: {model}")
        print(f"  - Instructions length: {len(instructions) if instructions else 0}")
        print(f"  - User input: {user_input[:50]}...")
        print(f"  - Conversation messages: {len(conversation_messages)}")
        print(f"  - Available tools: {len(available_tools)}")
        print(f"  - Needs web search: {needs_web_search}")

        ai_content = await create_response_with_responses_api_fallback(
            model, instructions, user_input, conversation_messages,
            available_tools, needs_web_search, model_config
        )

        print(f"âœ… Responses API success, content length: {len(ai_content) if ai_content else 0}")

        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜ - ì¤„ë°”ê¿ˆ ë³´ì¡´
        import re
        
        # ë‹¨ì–´ì™€ ê³µë°±/ì¤„ë°”ê¿ˆì„ ëª¨ë‘ ë³´ì¡´í•˜ë©´ì„œ í† í°í™”
        # \S+ëŠ” ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìë“¤(ë‹¨ì–´), \s+ëŠ” ê³µë°± ë¬¸ìë“¤(ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ ë“±)
        tokens = re.findall(r'\S+|\s+', ai_content)
        current_content = ""

        for i, token in enumerate(tokens):
            current_content += token

            # ê³µë°±/ì¤„ë°”ê¿ˆ í† í°ì€ ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•Šê³ , ë‹¨ì–´ í† í°ë§Œ ìŠ¤íŠ¸ë¦¬ë°
            if token.strip():  # ê³µë°±ì´ ì•„ë‹Œ í† í°(ë‹¨ì–´)ë§Œ ì „ì†¡
                chunk_data = {
                    "id": ai_message_id,
                    "content": token,
                    "role": "assistant",
                    "timestamp": datetime.now().isoformat(),
                    "sessionId": session_id,
                    "isComplete": False
                }

                yield f"data: {json.dumps(chunk_data)}\n\n"

                # ë‹¨ì–´ ê°„ ì•½ê°„ì˜ ë”œë ˆì´ (ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼)
                await asyncio.sleep(0.05)
            else:
                # ê³µë°±/ì¤„ë°”ê¿ˆ í† í°ë„ ì „ì†¡ (í¬ë§·íŒ… ë³´ì¡´ì„ ìœ„í•´)
                chunk_data = {
                    "id": ai_message_id,
                    "content": token,
                    "role": "assistant",
                    "timestamp": datetime.now().isoformat(),
                    "sessionId": session_id,
                    "isComplete": False
                }

                yield f"data: {json.dumps(chunk_data)}\n\n"

        # ì™„ë£Œ ì²­í¬
        final_chunk = {
            "id": ai_message_id,
            "content": "",
            "role": "assistant",
            "timestamp": datetime.now().isoformat(),
            "sessionId": session_id,
            "isComplete": True
        }
        yield f"data: {json.dumps(final_chunk)}\n\n"

    except Exception as e:
        print(f"ğŸš¨ Responses API streaming error: {e}")
        raise e


async def stream_with_chat_completions_fallback(
        model: str,
        conversation_messages: List[Dict],
        ai_message_id: str,
        session_id: str
):
    """Chat Completions APIë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° í´ë°±"""

    try:
        stream = await client.chat.completions.create(
            model=model,
            messages=conversation_messages,
            stream=True,
            max_tokens=4000,
            temperature=0.7
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content

                chunk_data = {
                    "id": ai_message_id,
                    "content": content,
                    "role": "assistant",
                    "timestamp": datetime.now().isoformat(),
                    "sessionId": session_id,
                    "isComplete": False
                }

                yield f"data: {json.dumps(chunk_data)}\n\n"

        # ì™„ë£Œ ì²­í¬
        final_chunk = {
            "id": ai_message_id,
            "content": "",
            "role": "assistant",
            "timestamp": datetime.now().isoformat(),
            "sessionId": session_id,
            "isComplete": True
        }
        yield f"data: {json.dumps(final_chunk)}\n\n"

    except Exception as e:
        print(f"ğŸš¨ Chat Completions streaming error: {e}")
        raise e


async def stream_with_realtime_api(request: ChatRequest, user_message: ChatMessage, model: str):
    """OpenAI Realtime APIë¥¼ ì‚¬ìš©í•œ ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°"""

    async def generate_realtime_stream():
        ai_message_id = generate_id()
        full_content = ""

        try:
            print(f"ğŸ™ï¸ Using Realtime API with model: {model}")

            async with client.beta.realtime.connect(model=model) as connection:
                # ì„¸ì…˜ ì„¤ì • (í…ìŠ¤íŠ¸ ëª¨ë“œ)
                await connection.session.update(session={'modalities': ['text']})

                # ëŒ€í™” ì•„ì´í…œ ìƒì„±
                await connection.conversation.item.create(
                    item={
                        "type": "message",
                        "role": "user",
                        "content": [{"type": "input_text", "text": request.content}],
                    }
                )

                # ì‘ë‹µ ìƒì„± ì‹œì‘
                await connection.response.create()

                # ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬
                async for event in connection:
                    if event.type == 'response.text.delta':
                        # ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ë¸íƒ€
                        delta_text = event.delta
                        full_content += delta_text

                        chunk = ChatStreamChunk(
                            id=ai_message_id,
                            content=delta_text,
                            role="assistant",
                            timestamp=datetime.now(),
                            sessionId=request.sessionId,
                            isComplete=False
                        )
                        yield f"data: {chunk.json()}\n\n"

                    elif event.type == 'response.text.done':
                        # í…ìŠ¤íŠ¸ ì™„ë£Œ
                        print(f"âœ… Realtime text complete")

                    elif event.type == 'response.done':
                        # ì „ì²´ ì‘ë‹µ ì™„ë£Œ
                        final_chunk = ChatStreamChunk(
                            id=ai_message_id,
                            content="",
                            role="assistant",
                            timestamp=datetime.now(),
                            sessionId=request.sessionId,
                            isComplete=True
                        )
                        yield f"data: {final_chunk.json()}\n\n"
                        break

                    elif event.type == 'error':
                        # ì—ëŸ¬ ì²˜ë¦¬
                        error_msg = f"Realtime API ì˜¤ë¥˜: {event.error.message}"
                        print(f"ğŸš¨ Realtime API Error: {error_msg}")

                        error_chunk = ChatStreamChunk(
                            id=ai_message_id,
                            content=error_msg,
                            role="assistant",
                            timestamp=datetime.now(),
                            sessionId=request.sessionId,
                            isComplete=True
                        )
                        yield f"data: {error_chunk.json()}\n\n"
                        break

            # AI ì‘ë‹µ ì €ì¥
            if full_content:
                ai_message = ChatMessage(
                    id=ai_message_id,
                    content=full_content,
                    role="assistant",
                    timestamp=datetime.now(),
                    sessionId=request.sessionId
                )

                messages_db[request.sessionId].append(ai_message.dict())
                update_session_message_count(request.sessionId)

        except Exception as e:
            error_msg = handle_openai_error(e, user_content=request.content)
            print(f"ğŸš¨ Realtime API Exception: {error_msg}")

            error_chunk = ChatStreamChunk(
                id=ai_message_id,
                content=error_msg,
                role="assistant",
                timestamp=datetime.now(),
                sessionId=request.sessionId,
                isComplete=True
            )
            yield f"data: {error_chunk.json()}\n\n"

    return StreamingResponse(
        generate_realtime_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )


async def stream_with_chat_completions(request: ChatRequest, user_message: ChatMessage, model: str):
    """Chat Completions APIë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° (í´ë°±)"""

    async def generate_chat_stream():
        ai_message_id = generate_id()
        full_content = ""

        try:
            print(f"ğŸ’¬ Using Chat Completions streaming with model: {model}")

            # ì„¸ì…˜ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ êµ¬ì„±
            session_messages = messages_db.get(request.sessionId, [])
            system_prompt = "ë‹¹ì‹ ì€ NSales Proì˜ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."

            conversation_messages = [{"role": "system", "content": system_prompt}]

            # ìµœê·¼ ë©”ì‹œì§€ë“¤ ì¶”ê°€
            recent_messages = session_messages[-20:] if len(session_messages) > 20 else session_messages
            for msg in recent_messages:
                conversation_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            conversation_messages.append({"role": "user", "content": request.content})

            # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
            stream = await client.chat.completions.create(
                model=model,
                messages=conversation_messages,
                max_tokens=2000,
                temperature=0.7,
                stream=True
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    delta_content = chunk.choices[0].delta.content
                    full_content += delta_content

                    chunk_obj = ChatStreamChunk(
                        id=ai_message_id,
                        content=delta_content,
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=request.sessionId,
                        isComplete=False
                    )
                    yield f"data: {chunk_obj.json()}\n\n"

                # ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì²´í¬
                if chunk.choices[0].finish_reason:
                    final_chunk = ChatStreamChunk(
                        id=ai_message_id,
                        content="",
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=request.sessionId,
                        isComplete=True
                    )
                    yield f"data: {final_chunk.json()}\n\n"
                    break

            # AI ì‘ë‹µ ì €ì¥
            if full_content:
                ai_message = ChatMessage(
                    id=ai_message_id,
                    content=full_content,
                    role="assistant",
                    timestamp=datetime.now(),
                    sessionId=request.sessionId
                )

                messages_db[request.sessionId].append(ai_message.dict())
                update_session_message_count(request.sessionId)

        except Exception as e:
            error_msg = handle_openai_error(e, user_content=request.content)
            print(f"ğŸš¨ Chat Completions Streaming Error: {error_msg}")

            error_chunk = ChatStreamChunk(
                id=ai_message_id,
                content=error_msg,
                role="assistant",
                timestamp=datetime.now(),
                sessionId=request.sessionId,
                isComplete=True
            )
            yield f"data: {error_chunk.json()}\n\n"

    return StreamingResponse(
        generate_chat_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )


# ì›ë˜ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ (ì„ì‹œ ë¹„í™œì„±í™”)
@app.post("/api/v1/chat/stream_disabled")
async def stream_chat_original(request: ChatRequest):
    if request.sessionId not in sessions_db:
        raise HTTPException(status_code=404, detail="Session not found")

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    user_message = ChatMessage(
        id=generate_id(),
        content=request.content,
        role="user",
        timestamp=datetime.now(),
        sessionId=request.sessionId
    )

    if request.sessionId not in messages_db:
        messages_db[request.sessionId] = []

    messages_db[request.sessionId].append(user_message.dict())

    async def generate_stream():
        ai_message_id = generate_id()
        full_content = ""

        # ì„¸ì…˜ì˜ ê¸°ì¡´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        session_messages = messages_db.get(request.sessionId, [])

        # í˜„ì¬ í•œêµ­ ì‹œê°„ ì •ë³´ ìƒì„±
        korea_tz = timezone(timedelta(hours=9))
        current_time_kst = datetime.now(korea_tz)

        # Google ì„œë¹„ìŠ¤ ì‚¬ìš© ì•ˆë‚´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""ë‹¹ì‹ ì€ NSales Proì˜ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì˜ì—… ë°ì´í„° ë¶„ì„, í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ, ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ë„ì›€ì„ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë¬¸ë§¥ì„ ìœ ì§€í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ìµœì‹  ì •ë³´ê°€ í•„ìš”í•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°, ë‰´ìŠ¤, ì‹œì¥ ë™í–¥ ë“±ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ì›¹ ê²€ìƒ‰ì„ ì ê·¹ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìµœì‹ ì˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
                        **í˜„ì¬ ì‹œê°„ ì •ë³´:**
                        - í˜„ì¬ ë‚ ì§œ: {current_time_kst.strftime('%Yë…„ %mì›” %dì¼ (%A)')}
                        - í˜„ì¬ ì‹œê°„: {current_time_kst.strftime('%Hì‹œ %Më¶„')}
                        - ì‹œê°„ëŒ€: í•œêµ­ í‘œì¤€ì‹œ (KST, UTC+9)
                        
                        "ì˜¤ëŠ˜", "ì´ë²ˆ ì£¼", "ì´ë²ˆ ë‹¬" ë“±ì˜ ì‹œê°„ í‘œí˜„ì„ ì‚¬ìš©í•  ë•ŒëŠ” ìœ„ì˜ í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ í•´ì„í•´ì£¼ì„¸ìš”.
                        """

        # ë©˜ì…˜ ê¸°ë°˜ ì„œë¹„ìŠ¤ í™œì„±í™” ë¡œì§
        mention_detected = False
        google_mention_keywords = ['@ìº˜ë¦°ë”', '@ë©”ì¼', '@ì¼ì •ìƒì„±', '@ë¹ˆì‹œê°„']

        for keyword in google_mention_keywords:
            if keyword in request.content:
                mention_detected = True
                break

        # Google ì„œë¹„ìŠ¤ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë©˜ì…˜ì´ ê°ì§€ëœ ê²½ìš° ì•ˆë‚´ ì¶”ê°€
        if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated() and mention_detected:
            print(f"ğŸ¯ Google ë©˜ì…˜ ê°ì§€ë¨: {request.content}")
            system_prompt += "\n\n**ğŸ¯ Google ì„œë¹„ìŠ¤ ë©˜ì…˜ ê°ì§€ë¨:**\nì‚¬ìš©ìê°€ @ë©˜ì…˜ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ë°˜ë“œì‹œ í˜¸ì¶œí•˜ì—¬ ìš”ì²­ì„ ì²˜ë¦¬í•˜ì„¸ìš”:\n- @ìº˜ë¦°ë” â†’ get_calendar_events í•¨ìˆ˜ í˜¸ì¶œ\n- @ë©”ì¼ â†’ get_emails ë˜ëŠ” send_email í•¨ìˆ˜ í˜¸ì¶œ\n- @ì¼ì •ìƒì„± â†’ create_calendar_event í•¨ìˆ˜ í˜¸ì¶œ\n- @ë¹ˆì‹œê°„ â†’ find_free_time í•¨ìˆ˜ í˜¸ì¶œ\n\në©˜ì…˜ì´ í¬í•¨ëœ ìš”ì²­ì€ ë°˜ë“œì‹œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."
        elif GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
            system_prompt += "\n\n**Google ì„œë¹„ìŠ¤ ì—°ë™ ì•ˆë‚´:**\nì‚¬ìš©ìê°€ ìº˜ë¦°ë”, ì¼ì •, ìŠ¤ì¼€ì¤„, Gmail, ì´ë©”ì¼ ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë©´ ë‹¤ìŒ í•¨ìˆ˜ë“¤ì„ ì ê·¹ í™œìš©í•˜ì„¸ìš”:\n- get_calendar_events: ìº˜ë¦°ë” ì¼ì • ì¡°íšŒ (ì˜¤ëŠ˜, ì´ë²ˆì£¼, ì´ë²ˆë‹¬ ë“±)\n- create_calendar_event: ìƒˆ ì¼ì • ìƒì„±\n- send_email: ì´ë©”ì¼ ì „ì†¡\n- get_emails: ì´ë©”ì¼ ì¡°íšŒ\n- find_free_time: ë¹ˆ ì‹œê°„ ì°¾ê¸°\n\nì‚¬ìš©ìê°€ 'ìº˜ë¦°ë”', 'ì¼ì •', 'ìŠ¤ì¼€ì¤„' ë“±ì˜ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë°˜ë“œì‹œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œê³µí•˜ì„¸ìš”."

        # OpenAI APIì— ì „ë‹¬í•  ë©”ì‹œì§€ êµ¬ì„±
        conversation_messages = [
            {"role": "system", "content": system_prompt}
        ]

        # ê¸°ì¡´ ëŒ€í™” ë‚´ìš© ì¶”ê°€ (ìµœê·¼ 20ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€í•˜ì—¬ í† í° ì ˆì•½)
        recent_messages = session_messages[-20:] if len(session_messages) > 20 else session_messages
        for msg in recent_messages:
            conversation_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })

        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        conversation_messages.append({"role": "user", "content": request.content})

        try:
            # ì„ íƒëœ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            selected_model = request.model if request.model in AVAILABLE_MODELS else "gpt-4o"
            model_config = AVAILABLE_MODELS[selected_model]

            print(f"Stream using model: {selected_model} ({model_config['name']})")
            print(f"Stream conversation length: {len(conversation_messages)} messages")  # ë””ë²„ê¹…ìš©

            # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ êµ¬ì„±
            available_tools = []
            print(f"ğŸ” Debug - mention_detected: {mention_detected}")
            print(f"ğŸ” Debug - GOOGLE_SERVICES_AVAILABLE: {GOOGLE_SERVICES_AVAILABLE}")
            print(
                f"ğŸ” Debug - is_authenticated: {auth_service.is_authenticated() if GOOGLE_SERVICES_AVAILABLE else 'N/A'}")

            # ì›¹ ê²€ìƒ‰ ì—¬ë¶€ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ê²°ì • (webSearch íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬)
            needs_web_search = getattr(request, 'webSearch', False)

            # ì›¹ ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€
            if needs_web_search and model_config["supports_web_search"]:
                available_tools.append({"type": "web_search"})

            # Google ì„œë¹„ìŠ¤ ë„êµ¬ ì¶”ê°€ (ì¸ì¦ëœ ê²½ìš°ë§Œ)
            if GOOGLE_SERVICES_AVAILABLE and auth_service.is_authenticated():
                available_tools.extend(GOOGLE_TOOLS)
                print(f"ğŸ”— Google ì„œë¹„ìŠ¤ ë„êµ¬ {len(GOOGLE_TOOLS)}ê°œ ì¶”ê°€ë¨")
                print(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤: {[tool['function']['name'] for tool in GOOGLE_TOOLS]}")
            search_content = request.content

            if needs_web_search and model_config["supports_web_search"]:
                print("ğŸ” Web search detected in stream - using Responses API")
                
                # ì›¹ ê²€ìƒ‰ ì‹œì‘ ìƒíƒœ í‘œì‹œ
                search_start_chunk = ChatStreamChunk(
                    id=ai_message_id,
                    content="ğŸ” ì›¹ ê²€ìƒ‰ ì¤‘...",
                    role="assistant",
                    timestamp=datetime.now(),
                    sessionId=request.sessionId,
                    isComplete=False
                )
                yield f"data: {search_start_chunk.json()}\n\n"
                await asyncio.sleep(0.1)
                
                try:
                    # ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì¼ë°˜ ì‘ë‹µ ì‚¬ìš© (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
                    context_input = build_context_input(search_content, conversation_messages)
                    response = await client.responses.create(
                        model=selected_model,
                        instructions=system_prompt,
                        input=context_input,
                        tools=[
                            {
                                "type": "web_search"
                            }
                        ]
                    )

                    # Extract message content from output
                    ai_content = ""
                    sources = []

                    for output_item in response.output:
                        if output_item.type == 'message' and hasattr(output_item, 'content'):
                            for content_item in output_item.content:
                                if content_item.type == 'output_text':
                                    ai_content += content_item.text

                                    # Extract URL citations from annotations
                                    if hasattr(content_item, 'annotations'):
                                        for annotation in content_item.annotations:
                                            if annotation.type == 'url_citation':
                                                sources.append({
                                                    'title': getattr(annotation, 'title', ''),
                                                    'url': getattr(annotation, 'url', ''),
                                                    'snippet': ''
                                                })

                    # Add sources to the content if found
                    if sources:
                        sources_text = "\n\n## ì°¸ê³  ì¶œì²˜\n\n"
                        for i, source in enumerate(sources, 1):
                            title = source['title'] if source['title'] else f"ì¶œì²˜ {i}"
                            sources_text += f"{i}. **[{title}]({source['url']})**\n"
                        ai_content += sources_text
                        print(f"ğŸ“š Found {len(sources)} web search sources in stream")

                    full_content = ai_content


                    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë° (ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë³´ì¡´)
                    web_search_chunk = ChatStreamChunk(
                        id=ai_message_id,
                        content=ai_content,
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=request.sessionId,
                        isComplete=False
                    )
                    yield f"data: {web_search_chunk.json()}\n\n"
                    await asyncio.sleep(0.5)  # ì›¹ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ ì‹œê°„

                    # ì›¹ ê²€ìƒ‰ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸
                    final_chunk = ChatStreamChunk(
                        id=ai_message_id,
                        content="",
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=request.sessionId,
                        isComplete=True
                    )
                    yield f"data: {final_chunk.json()}\\n\\n"

                    # ì›¹ ê²€ìƒ‰ ì„±ê³µ ì‹œ ì—¬ê¸°ì„œ return
                    # AI ì‘ë‹µ ì €ì¥
                    ai_message = ChatMessage(
                        id=ai_message_id,
                        content=full_content,
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=request.sessionId
                    )

                    messages_db[request.sessionId].append(ai_message.dict())
                    update_session_message_count(request.sessionId)
                    return

                except Exception as web_error:
                    print(f"Responses API error in stream, falling back to chat completions: {web_error}")
                    # ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í´ë°±
                    # ìŠ¤íŠ¸ë¦¬ë° íŒŒë¼ë¯¸í„° êµ¬ì„±
                    stream_params = {
                        "model": selected_model,
                        "messages": conversation_messages,
                        "max_tokens": model_config["max_tokens"],
                        "temperature": model_config["temperature"],
                        "stream": True
                    }

                    # ë„êµ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                    if available_tools:
                        stream_params["tools"] = available_tools
                        # ë©˜ì…˜ì´ ê°ì§€ëœ ê²½ìš° Function Calling ê°•ì œ í™œì„±í™”
                        if mention_detected:
                            stream_params["tool_choice"] = {"type": "function",
                                                            "function": {"name": "get_calendar_events"}}
                            print(f"ğŸ¯ ê°•ì œ Function Calling í™œì„±í™”: get_calendar_events")
                        else:
                            stream_params["tool_choice"] = "auto"
                        print(f"ğŸ› ï¸ ìŠ¤íŠ¸ë¦¬ë° Function Calling í™œì„±í™”: {len(available_tools)}ê°œ ë„êµ¬")

                    stream = await client.chat.completions.create(**stream_params)
            else:
                # ì„ì‹œë¡œ ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì¼ë°˜ API ì‚¬ìš©
                chat_params = {
                    "model": selected_model,
                    "messages": conversation_messages,
                    "max_tokens": model_config["max_tokens"],
                    "temperature": model_config["temperature"]
                }

                # ë„êµ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€ (ì›¹ ê²€ìƒ‰ ë„êµ¬ ë˜ëŠ” Google ë„êµ¬)
                if available_tools:
                    chat_params["tools"] = available_tools
                    # ë©˜ì…˜ì´ ê°ì§€ëœ ê²½ìš° Function Calling ê°•ì œ í™œì„±í™”
                    if mention_detected:
                        chat_params["tool_choice"] = {"type": "function", "function": {"name": "get_calendar_events"}}
                        print(f"ğŸ¯ ê°•ì œ Function Calling í™œì„±í™”: get_calendar_events")
                    else:
                        chat_params["tool_choice"] = "auto"
                    print(f"ğŸ› ï¸ Function Calling í™œì„±í™”: {len(available_tools)}ê°œ ë„êµ¬")

                print(f"ğŸ” ë¹„ìŠ¤íŠ¸ë¦¬ë° íŒŒë¼ë¯¸í„°: {chat_params}")
                response = await client.chat.completions.create(**chat_params)

                # ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë³€í™˜
                ai_content = response.choices[0].message.content

                # Function callsê°€ ìˆëŠ”ì§€ í™•ì¸
                if response.choices[0].message.tool_calls:
                    print(f"ğŸ”§ Function í˜¸ì¶œ ê°ì§€: {len(response.choices[0].message.tool_calls)}ê°œ")

                    for tool_call in response.choices[0].message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)

                        if function_name in FUNCTION_MAP:
                            try:
                                # í•¨ìˆ˜ ì‹¤í–‰ ìƒíƒœ í‘œì‹œ (í•œ ë²ˆì— ì¶œë ¥)
                                status_content = f"\n\nğŸ”„ {function_name} ì‹¤í–‰ ì¤‘...\n"
                                status_chunk = ChatStreamChunk(
                                    id=ai_message_id,
                                    content=status_content,
                                    role="assistant",
                                    timestamp=datetime.now(),
                                    sessionId=request.sessionId,
                                    isComplete=False,
                                    functionCall=function_name,
                                    functionStatus="running"
                                )
                                yield f"data: {status_chunk.json()}\n\n"
                                await asyncio.sleep(0.1)

                                # í•¨ìˆ˜ ì‹¤í–‰
                                function_result = FUNCTION_MAP[function_name](**function_args)

                                # ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¶œë ¥
                                if isinstance(function_result, (dict, list)):
                                    result_content = f"## âœ… {function_name} ì‹¤í–‰ ì™„ë£Œ\n\n```json\n{json.dumps(function_result, ensure_ascii=False, indent=2)}\n```\n\n"
                                else:
                                    result_content = f"## âœ… {function_name} ì‹¤í–‰ ì™„ë£Œ\n\n{function_result}\n\n"
                                ai_content += result_content

                                # í•¨ìˆ˜ ê²°ê³¼ëŠ” í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë° (ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê°œì„ )
                                result_chunk = ChatStreamChunk(
                                    id=ai_message_id,
                                    content=result_content,
                                    role="assistant",
                                    timestamp=datetime.now(),
                                    sessionId=request.sessionId,
                                    isComplete=False,
                                    functionCall=function_name,
                                    functionStatus="completed"
                                )
                                yield f"data: {result_chunk.json()}\n\n"
                                await asyncio.sleep(0.2)

                            except Exception as e:
                                error_content = f"## âŒ {function_name} ì‹¤í–‰ ì˜¤ë¥˜\n\n{str(e)}\n\n"
                                ai_content += error_content

                                # ì—ëŸ¬ ë‚´ìš©ì€ í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë° (ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê°œì„ )
                                error_chunk = ChatStreamChunk(
                                    id=ai_message_id,
                                    content=error_content,
                                    role="assistant",
                                    timestamp=datetime.now(),
                                    sessionId=request.sessionId,
                                    isComplete=False,
                                    functionCall=function_name,
                                    functionStatus="error"
                                )
                                yield f"data: {error_chunk.json()}\n\n"
                                await asyncio.sleep(0.2)
                else:
                    # ì¼ë°˜ ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° (ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê°œì„ )
                    import re
                    sentences = re.split(r'(\. |\? |\! |\n\n|\n)', ai_content)
                    
                    for sentence in sentences:
                        if sentence.strip():
                            stream_chunk = ChatStreamChunk(
                                id=ai_message_id,
                                content=sentence,
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=request.sessionId,
                                isComplete=False
                            )
                            yield f"data: {stream_chunk.json()}\n\n"
                            
                            # ë¬¸ì¥ ìœ í˜•ì— ë”°ë¥¸ ì ì‘ì  ì§€ì—°
                            if sentence.endswith(('.', '!', '?')):
                                await asyncio.sleep(0.3)
                            elif sentence == '\n\n':
                                await asyncio.sleep(0.2)
                            elif sentence == '\n':
                                await asyncio.sleep(0.15)
                            else:
                                await asyncio.sleep(0.1)

                full_content = ai_content

                # ì™„ë£Œ ì‹ í˜¸
                final_chunk = ChatStreamChunk(
                    id=ai_message_id,
                    content="",
                    role="assistant",
                    timestamp=datetime.now(),
                    sessionId=request.sessionId,
                    isComplete=True
                )
                yield f"data: {final_chunk.json()}\n\n"

                # AI ì‘ë‹µ ì €ì¥
                ai_message = ChatMessage(
                    id=ai_message_id,
                    content=full_content,
                    role="assistant",
                    timestamp=datetime.now(),
                    sessionId=request.sessionId
                )

                messages_db[request.sessionId].append(ai_message.dict())
                update_session_message_count(request.sessionId)
                return

            # ì¼ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°)
            tool_calls = []
            current_tool_call = None

            async for chunk in stream:
                delta = chunk.choices[0].delta

                # Function calling ì²˜ë¦¬
                if delta.tool_calls:
                    for tool_call_delta in delta.tool_calls:
                        if tool_call_delta.index == 0:
                            if current_tool_call is None:
                                current_tool_call = {
                                    "id": tool_call_delta.id or "",
                                    "function": {
                                        "name": tool_call_delta.function.name or "",
                                        "arguments": tool_call_delta.function.arguments or ""
                                    }
                                }
                            else:
                                if tool_call_delta.function.arguments:
                                    current_tool_call["function"]["arguments"] += tool_call_delta.function.arguments

                # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
                elif delta.content:
                    content = delta.content
                    full_content += content

                    stream_chunk = ChatStreamChunk(
                        id=ai_message_id,
                        content=content,
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=request.sessionId,
                        isComplete=False
                    )

                    yield f"data: {stream_chunk.json()}\n\n"
                    await asyncio.sleep(0.01)

                # ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì²´í¬
                if chunk.choices[0].finish_reason == "tool_calls" and current_tool_call:
                    tool_calls.append(current_tool_call)

            # Function í˜¸ì¶œ ì‹¤í–‰
            if tool_calls:
                print(f"ğŸ”§ Function í˜¸ì¶œ ì‹¤í–‰: {len(tool_calls)}ê°œ")

                for tool_call in tool_calls:
                    function_name = tool_call["function"]["name"]
                    function_args = json.loads(tool_call["function"]["arguments"])

                    if function_name in FUNCTION_MAP:
                        try:
                            # í•¨ìˆ˜ ì‹¤í–‰ ìƒíƒœ í‘œì‹œ
                            status_chunk = ChatStreamChunk(
                                id=ai_message_id,
                                content=f"\n\nğŸ”„ {function_name} ì‹¤í–‰ ì¤‘...\n",
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=request.sessionId,
                                isComplete=False,
                                functionCall=function_name,
                                functionStatus="running"
                            )
                            yield f"data: {status_chunk.json()}\n\n"

                            # í•¨ìˆ˜ ì‹¤í–‰
                            function_result = FUNCTION_MAP[function_name](**function_args)

                            # ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¶œë ¥
                            if isinstance(function_result, (dict, list)):
                                result_content = f"## âœ… {function_name} ì‹¤í–‰ ì™„ë£Œ\n\n```json\n{json.dumps(function_result, ensure_ascii=False, indent=2)}\n```\n\n"
                            else:
                                result_content = f"## âœ… {function_name} ì‹¤í–‰ ì™„ë£Œ\n\n{function_result}\n\n"
                            full_content += result_content

                            # í•¨ìˆ˜ ê²°ê³¼ëŠ” í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë° (ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê°œì„ )
                            result_chunk = ChatStreamChunk(
                                id=ai_message_id,
                                content=result_content,
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=request.sessionId,
                                isComplete=False,
                                functionCall=function_name,
                                functionStatus="completed"
                            )
                            yield f"data: {result_chunk.json()}\n\n"
                            await asyncio.sleep(0.2)

                        except Exception as e:
                            error_content = f"## âŒ {function_name} ì‹¤í–‰ ì˜¤ë¥˜\n\n{str(e)}\n\n"
                            full_content += error_content

                            # ì—ëŸ¬ ë‚´ìš©ì€ í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë° (ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê°œì„ )
                            error_chunk = ChatStreamChunk(
                                id=ai_message_id,
                                content=error_content,
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=request.sessionId,
                                isComplete=False,
                                functionCall=function_name,
                                functionStatus="error"
                            )
                            yield f"data: {error_chunk.json()}\n\n"
                            await asyncio.sleep(0.2)

            # ì™„ë£Œ ì‹ í˜¸
            final_chunk = ChatStreamChunk(
                id=ai_message_id,
                content="",
                role="assistant",
                timestamp=datetime.now(),
                sessionId=request.sessionId,
                isComplete=True
            )
            yield f"data: {final_chunk.json()}\n\n"

        except Exception as e:
            # OpenAI API ì˜¤ë¥˜ ì‹œ í´ë°± ì‘ë‹µ
            print(f"ğŸš¨ ìŠ¤íŠ¸ë¦¬ë° API ì˜ˆì™¸ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            fallback_content = f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. '{request.content}'ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            full_content = fallback_content

            # í´ë°± ì‘ë‹µì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            words = fallback_content.split()
            for i, word in enumerate(words):
                stream_chunk = ChatStreamChunk(
                    id=ai_message_id,
                    content=word + " ",
                    role="assistant",
                    timestamp=datetime.now(),
                    sessionId=request.sessionId,
                    isComplete=i == len(words) - 1
                )
                yield f"data: {stream_chunk.json()}\n\n"
                await asyncio.sleep(0.1)

        # AI ì‘ë‹µ ì €ì¥
        ai_message = ChatMessage(
            id=ai_message_id,
            content=full_content,
            role="assistant",
            timestamp=datetime.now(),
            sessionId=request.sessionId
        )

        messages_db[request.sessionId].append(ai_message.dict())
        update_session_message_count(request.sessionId)

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )


@app.delete("/api/v1/chat/messages/{message_id}")
async def delete_message(message_id: str):
    for session_id, messages in messages_db.items():
        for i, msg in enumerate(messages):
            if msg["id"] == message_id:
                del messages[i]
                update_session_message_count(session_id)
                return {"message": "Message deleted successfully"}

    raise HTTPException(status_code=404, detail="Message not found")


@app.post("/api/v1/chat/messages/{message_id}/regenerate", response_model=ChatResponse)
async def regenerate_message(message_id: str):
    for session_id, messages in messages_db.items():
        for i, msg in enumerate(messages):
            if msg["id"] == message_id and msg["role"] == "assistant":
                # ì´ì „ ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ê¸°
                user_message = None
                if i > 0:
                    user_message = messages[i - 1]

                if not user_message:
                    raise HTTPException(status_code=400, detail="Cannot regenerate: no previous user message found")

                try:
                    # ì„¸ì…˜ì˜ ê¸°ì¡´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì¬ìƒì„±í•  ë©”ì‹œì§€ ì œì™¸)
                    session_messages = messages_db.get(session_id, [])

                    # OpenAI APIì— ì „ë‹¬í•  ë©”ì‹œì§€ êµ¬ì„±
                    conversation_messages = [
                        {"role": "system",
                         "content": "ë‹¹ì‹ ì€ NSales Proì˜ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì˜ì—… ë°ì´í„° ë¶„ì„, í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ, ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ë„ì›€ì„ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë¬¸ë§¥ì„ ìœ ì§€í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."}
                    ]

                    # ì¬ìƒì„±í•  ë©”ì‹œì§€ ì´ì „ê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš© ì¶”ê°€
                    for j, msg in enumerate(session_messages):
                        if j >= i:  # ì¬ìƒì„±í•  ë©”ì‹œì§€ë¶€í„°ëŠ” ì œì™¸
                            break
                        conversation_messages.append({
                            "role": msg["role"],
                            "content": msg["content"]
                        })

                    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
                    conversation_messages.append({"role": "user", "content": user_message["content"]})

                    # OpenAI API í˜¸ì¶œ
                    response = await client.chat.completions.create(
                        model="gpt-4o",
                        messages=conversation_messages,
                        max_tokens=1000,
                        temperature=0.8  # ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìœ„í•´ temperature ì¦ê°€
                    )

                    new_content = response.choices[0].message.content

                except Exception as e:
                    # OpenAI API ì˜¤ë¥˜ ì‹œ í´ë°± ì‘ë‹µ
                    new_content = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. '{user_message['content']}'ì— ëŒ€í•œ ìƒˆë¡œìš´ ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤."

                # ìƒˆë¡œìš´ ì‘ë‹µìœ¼ë¡œ êµì²´
                new_message = ChatMessage(
                    id=generate_id(),
                    content=new_content,
                    role="assistant",
                    timestamp=datetime.now(),
                    sessionId=session_id
                )

                messages[i] = new_message.dict()
                update_session_message_count(session_id)

                return ChatResponse(**new_message.dict())

    raise HTTPException(status_code=404, detail="Message not found")


# ===========================
# ğŸ—‚ï¸ ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
# ===========================

@app.get("/api/v1/vector-stores")
async def list_available_vector_stores():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„° ìŠ¤í† ì–´ ëª©ë¡ ì¡°íšŒ"""
    try:
        stores = await list_vector_stores()
        return {
            "success": True,
            "vector_stores": stores,
            "total_count": len(stores)
        }
    except Exception as e:
        logger.error(f"âŒ Failed to list vector stores: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ìŠ¤í† ì–´ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/v1/sessions/{session_id}/vector-store")
async def create_session_vector_store(session_id: str, name: str = None):
    """ì„¸ì…˜ë³„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    try:
        # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
        if session_id not in sessions_db:
            raise HTTPException(status_code=404, detail="Session not found")

        vector_store_id = await create_or_get_vector_store(session_id, name)
        return {
            "success": True,
            "vector_store_id": vector_store_id,
            "session_id": session_id,
            "message": "ë²¡í„° ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        logger.error(f"âŒ Failed to create vector store for session {session_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {str(e)}")


@app.post("/api/v1/sessions/{session_id}/vector-store/search")
async def search_session_vector_store(session_id: str, query: str, limit: int = 5):
    """ì„¸ì…˜ë³„ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰"""
    try:
        # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
        if session_id not in sessions_db:
            raise HTTPException(status_code=404, detail="Session not found")

        # ë²¡í„° ìŠ¤í† ì–´ í™•ì¸
        if session_id not in vector_stores_db:
            raise HTTPException(status_code=404, detail="Vector store not found for this session")

        vector_store_id = vector_stores_db[session_id]
        search_results = await search_vector_store(vector_store_id, query, limit)

        return {
            "success": True,
            "query": query,
            "results": search_results,
            "total_results": len(search_results)
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Failed to search vector store for session {session_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


@app.post("/api/v1/knowledge-base/create")
async def create_knowledge_base(documents: List[str], session_id: str = None):
    """ì§€ì‹ ë² ì´ìŠ¤ ìƒì„± (ë¬¸ì„œ ëª©ë¡ìœ¼ë¡œë¶€í„°)"""
    try:
        if not documents:
            raise HTTPException(status_code=400, detail="Documents list cannot be empty")

        vector_store_id = await create_knowledge_base_embeddings(documents, session_id)
        return {
            "success": True,
            "vector_store_id": vector_store_id,
            "documents_count": len(documents),
            "session_id": session_id,
            "message": "ì§€ì‹ ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        logger.error(f"âŒ Failed to create knowledge base: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì§€ì‹ ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")


async def stream_with_direct_function_calling(
        session_id: str,
        model: str,
        conversation_messages: List[Dict],
        available_tools: List[Dict],
        model_config: Dict,
        user_message: ChatMessage
):
    """ë©˜ì…˜ ê°ì§€ ì‹œ ì§ì ‘ Function Calling ì‹¤í–‰"""

    async def generate_direct_function_stream():
        ai_message_id = generate_id()
        full_content = ""

        try:
            print(f"ğŸ¯ Direct Function Calling - Model: {model}")
            print(f"ğŸ”§ Available tools: {[tool['function']['name'] for tool in available_tools]}")

            # ìë™ Function Calling (OpenAIê°€ ì ì ˆí•œ í•¨ìˆ˜ ì„ íƒ)
            chat_params = {
                "model": model,
                "messages": conversation_messages,
                "max_tokens": model_config["max_tokens"],
                "temperature": model_config["temperature"],
                "tools": available_tools,
                "tool_choice": "auto",  # Let OpenAI choose the appropriate function
                "stream": True
            }

            print(f"ğŸš€ Creating stream with auto Function Calling...")
            print(f"ğŸ“ Last user message: {user_message.content}")
            stream = await client.chat.completions.create(**chat_params)

            tool_calls = []
            current_tool_call = None

            async for chunk in stream:
                delta = chunk.choices[0].delta
                finish_reason = chunk.choices[0].finish_reason

                print(
                    f"ğŸ”„ Stream chunk - finish_reason: {finish_reason}, delta: content={bool(delta.content)}, tool_calls={bool(delta.tool_calls)}")

                # ì¼ë°˜ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì²˜ë¦¬
                if delta.content:
                    print(f"ğŸ“ Content chunk: {delta.content[:50]}...")
                    content_chunk = ChatStreamChunk(
                        id=ai_message_id,
                        content=delta.content,
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=session_id,
                        isComplete=False
                    )
                    yield f"data: {content_chunk.json()}\n\n"
                    full_content += delta.content

                # Function Calling ì²˜ë¦¬
                if delta.tool_calls:
                    print(f"ğŸ”§ Tool call delta detected: {delta.tool_calls}")
                    for tool_call_delta in delta.tool_calls:
                        if tool_call_delta.index == 0:
                            if current_tool_call is None:
                                current_tool_call = {
                                    "id": tool_call_delta.id or "",
                                    "function": {
                                        "name": tool_call_delta.function.name or "",
                                        "arguments": tool_call_delta.function.arguments or ""
                                    }
                                }
                                print(f"ğŸ†• New tool call: {current_tool_call['function']['name']}")
                            else:
                                # í•¨ìˆ˜ arguments ëˆ„ì 
                                current_tool_call["function"]["arguments"] += tool_call_delta.function.arguments or ""
                                print(
                                    f"ğŸ“ Accumulating arguments: {current_tool_call['function']['arguments'][:100]}...")

                # ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì²´í¬
                if finish_reason == "tool_calls" and current_tool_call:
                    print(f"âœ… Tool calls completed: {current_tool_call}")
                    tool_calls.append(current_tool_call)
                    break
                elif finish_reason == "stop":
                    print("â¹ï¸ Stream finished with stop")
                    break

            # Function í˜¸ì¶œ ì‹¤í–‰
            if tool_calls:
                print(f"ğŸ”§ Executing {len(tool_calls)} function calls")

                for tool_call in tool_calls:
                    function_name = tool_call["function"]["name"]
                    function_args = json.loads(tool_call["function"]["arguments"])

                    print(f"ğŸ“ Calling function: {function_name}({function_args})")

                    if function_name in FUNCTION_MAP:
                        try:
                            # ì‹¤í–‰ ìƒíƒœ í‘œì‹œ
                            status_chunk = ChatStreamChunk(
                                id=ai_message_id,
                                content=f"\n\nğŸ”„ {function_name} ì‹¤í–‰ ì¤‘...\n",
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=session_id,
                                isComplete=False,
                                functionCall=function_name,
                                functionStatus="running"
                            )
                            yield f"data: {status_chunk.json()}\n\n"

                            # í•¨ìˆ˜ ì‹¤í–‰
                            function_result = FUNCTION_MAP[function_name](**function_args)

                            # êµ¬ì¡°í™”ëœ ê²°ê³¼ ìƒì„±
                            structured_result = {
                                "type": "function_result",
                                "function_name": function_name,
                                "result": function_result,
                                "timestamp": datetime.now().isoformat()
                            }

                            # UIì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°í™”ëœ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                            result_content = f"\n## ğŸ”§ {function_name} ì‹¤í–‰ ê²°ê³¼\n\n"
                            if isinstance(function_result, dict) or isinstance(function_result, list):
                                result_content += f"```json\n{json.dumps(function_result, indent=2, ensure_ascii=False)}\n```\n\n"
                            else:
                                result_content += f"{function_result}\n\n"
                            full_content += result_content

                            # ê²°ê³¼ë¥¼ í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë°
                            result_chunk = ChatStreamChunk(
                                id=ai_message_id,
                                content=result_content,
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=session_id,
                                isComplete=False,
                                functionCall=function_name,
                                functionStatus="completed"
                            )
                            yield f"data: {result_chunk.json()}\n\n"

                        except Exception as e:
                            error_content = f"## âŒ {function_name} ì‹¤í–‰ ì˜¤ë¥˜\n\n{str(e)}\n\n"
                            full_content += error_content

                            # ì—ëŸ¬ ë‚´ìš©ì€ í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë° (ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê°œì„ )
                            error_chunk = ChatStreamChunk(
                                id=ai_message_id,
                                content=error_content,
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=session_id,
                                isComplete=False,
                                functionCall=function_name,
                                functionStatus="error"
                            )
                            yield f"data: {error_chunk.json()}\n\n"
                            await asyncio.sleep(0.2)

            # ì™„ë£Œ ì‹ í˜¸
            final_chunk = ChatStreamChunk(
                id=ai_message_id,
                content="",
                role="assistant",
                timestamp=datetime.now(),
                sessionId=session_id,
                isComplete=True
            )
            yield f"data: {final_chunk.json()}\n\n"

            # AI ì‘ë‹µ ì €ì¥
            ai_message = ChatMessage(
                id=ai_message_id,
                content=full_content,
                role="assistant",
                timestamp=datetime.now(),
                sessionId=session_id
            )
            messages_db[session_id].append(ai_message.dict())
            update_session_message_count(session_id)

        except Exception as e:
            error_msg = f"## âŒ Direct Function Calling ì˜¤ë¥˜\n\n{str(e)}"
            print(error_msg)

            error_chunk = ChatStreamChunk(
                id=ai_message_id,
                content=error_msg,
                role="assistant",
                timestamp=datetime.now(),
                sessionId=session_id,
                isComplete=True
            )
            yield f"data: {error_chunk.json()}\n\n"

    return StreamingResponse(
        generate_direct_function_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )


# ===========================
# ğŸ› ï¸ ìƒˆë¡œìš´ AI Tools ì‹œìŠ¤í…œ API
# ===========================

@app.get("/api/v1/tools/status")
async def get_tools_status():
    """AI Tools ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    if not TOOLS_SYSTEM_AVAILABLE:
        return {"available": False, "error": "Tools ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    status = tool_manager.get_status()
    tools_info = tool_manager.registry.get_available_tools_info()

    return {
        "available": True,
        "status": status,
        "tools": tools_info,
        "google_auth_status": auth_service.is_authenticated() if GOOGLE_SERVICES_AVAILABLE else False
    }


@app.get("/api/v1/tools/list")
async def list_available_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë„êµ¬ ëª©ë¡ ì¡°íšŒ"""
    if not TOOLS_SYSTEM_AVAILABLE:
        raise HTTPException(status_code=503, detail="Tools ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return {
        "tools": tool_manager.registry.get_available_tools_info(),
        "schemas": tool_manager.registry.get_openai_schemas()
    }


class EnhancedChatRequest(BaseModel):
    message: str
    sessionId: Optional[str] = None
    model: str = "gpt-4o"
    use_tools: bool = True
    tool_categories: Optional[List[str]] = None  # ["calendar", "email", "crm", "utility"]


class EnhancedChatStreamChunk(BaseModel):
    id: str
    content: str
    role: str
    timestamp: datetime
    sessionId: str
    isComplete: bool = False
    # Tools ê´€ë ¨ í•„ë“œ
    toolCall: Optional[str] = None
    toolStatus: Optional[str] = None  # "running", "completed", "error"
    toolResult: Optional[Dict] = None


@app.post("/api/v1/chat/enhanced")
async def enhanced_chat_stream(request: EnhancedChatRequest):
    """ìƒˆë¡œìš´ AI Tools ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ëŠ” í–¥ìƒëœ ì±„íŒ… API"""

    if not TOOLS_SYSTEM_AVAILABLE:
        raise HTTPException(status_code=503, detail="Tools ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    session_id = request.sessionId or str(uuid.uuid4())

    # í˜„ì¬ í•œêµ­ ì‹œê°„ ì •ë³´ ìƒì„±
    korea_tz = timezone(timedelta(hours=9))
    current_time_kst = datetime.now(korea_tz)

    # Google ì„œë¹„ìŠ¤ ì‚¬ìš© ì•ˆë‚´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = f"""ë‹¹ì‹ ì€ NSales Proì˜ ì˜ì—… AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì˜ì—… ë°ì´í„° ë¶„ì„, í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ, ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ë„ì›€ì„ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë¬¸ë§¥ì„ ìœ ì§€í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ìµœì‹  ì •ë³´ê°€ í•„ìš”í•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°, ë‰´ìŠ¤, ì‹œì¥ ë™í–¥ ë“±ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ì›¹ ê²€ìƒ‰ì„ ì ê·¹ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìµœì‹ ì˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

**í˜„ì¬ ì‹œê°„ ì •ë³´:**
- í˜„ì¬ ë‚ ì§œ: {current_time_kst.strftime('%Yë…„ %mì›” %dì¼ (%A)')}
- í˜„ì¬ ì‹œê°„: {current_time_kst.strftime('%Hì‹œ %Më¶„')}
- ì‹œê°„ëŒ€: í•œêµ­ í‘œì¤€ì‹œ (KST, UTC+9)

"ì˜¤ëŠ˜", "ì´ë²ˆ ì£¼", "ì´ë²ˆ ë‹¬" ë“±ì˜ ì‹œê°„ í‘œí˜„ì„ ì‚¬ìš©í•  ë•ŒëŠ” ìœ„ì˜ í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ í•´ì„í•´ì£¼ì„¸ìš”."""

    # ì„¸ì…˜ ì´ˆê¸°í™”
    if session_id not in messages_db:
        messages_db[session_id] = []
        sessions_db[session_id] = {
            "id": session_id,
            "createdAt": datetime.now(),
            "messageCount": 0,
            "model": request.model
        }

    async def generate_enhanced_stream():
        ai_message_id = str(uuid.uuid4())
        full_content = ""

        try:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            user_message = ChatMessage(
                id=str(uuid.uuid4()),
                content=request.message,
                role="user",
                timestamp=datetime.now(),
                sessionId=session_id
            )
            messages_db[session_id].append(user_message.dict())

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
            conversation_messages = [{"role": "system", "content": system_prompt}]
            for msg in messages_db[session_id]:
                conversation_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

            # ì‚¬ìš©í•  ë„êµ¬ë“¤ ì„ íƒ
            available_tools = []
            if request.use_tools:
                if request.tool_categories:
                    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë„êµ¬ë§Œ ì‚¬ìš©
                    for category in request.tool_categories:
                        tools_in_category = tool_manager.registry.get_tools_by_category(category)
                        available_tools.extend([tool.get_schema() for tool in tools_in_category])
                else:
                    # ëª¨ë“  ë„êµ¬ ì‚¬ìš©
                    available_tools = tool_manager.registry.get_openai_schemas()

            print(f"ğŸ› ï¸ Using {len(available_tools)} tools for enhanced chat")

            # OpenAI Chat Completions API í˜¸ì¶œ
            response = await client.chat.completions.create(
                model=request.model,
                messages=conversation_messages,
                tools=available_tools if available_tools else None,
                stream=True,
                temperature=0.7
            )

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            tool_calls = []
            current_tool_call = None

            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content_piece = chunk.choices[0].delta.content
                    full_content += content_piece

                    # ë‚´ìš© ìŠ¤íŠ¸ë¦¬ë°
                    stream_chunk = EnhancedChatStreamChunk(
                        id=ai_message_id,
                        content=content_piece,
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=session_id,
                        isComplete=False
                    )
                    yield f"data: {stream_chunk.json()}\n\n"

                # Tool calls ê°ì§€
                if chunk.choices[0].delta.tool_calls:
                    for tool_call_delta in chunk.choices[0].delta.tool_calls:
                        if tool_call_delta.index is not None:
                            # ìƒˆë¡œìš´ tool call ì‹œì‘
                            if len(tool_calls) <= tool_call_delta.index:
                                tool_calls.append({
                                    "id": "",
                                    "type": "function",
                                    "function": {"name": "", "arguments": ""}
                                })

                            current_tool_call = tool_calls[tool_call_delta.index]

                            if tool_call_delta.id:
                                current_tool_call["id"] = tool_call_delta.id
                            if tool_call_delta.function.name:
                                current_tool_call["function"]["name"] = tool_call_delta.function.name
                            if tool_call_delta.function.arguments:
                                current_tool_call["function"]["arguments"] += tool_call_delta.function.arguments

            # Tool calls ì‹¤í–‰
            if tool_calls:
                print(f"ğŸ”§ Executing {len(tool_calls)} tool calls")

                for tool_call in tool_calls:
                    function_name = tool_call["function"]["name"]

                    # ì‹¤í–‰ ì‹œì‘ ì•Œë¦¼
                    start_chunk = EnhancedChatStreamChunk(
                        id=ai_message_id,
                        content=f"\n\nğŸ”„ {function_name} ì‹¤í–‰ ì¤‘...\n",
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=session_id,
                        isComplete=False,
                        toolCall=function_name,
                        toolStatus="running"
                    )
                    yield f"data: {start_chunk.json()}\n\n"

                    try:
                        # ìƒˆë¡œìš´ Tools ì‹œìŠ¤í…œìœ¼ë¡œ ì‹¤í–‰
                        mock_tool_call = type('MockToolCall', (), {
                            'function': type('MockFunction', (), {
                                'name': function_name,
                                'arguments': tool_call["function"]["arguments"]
                            })()
                        })()

                        result = await tool_manager.registry.execute_tool_call(mock_tool_call)

                        # ê²°ê³¼ íŒŒì‹±
                        try:
                            result_data = json.loads(result)
                        except:
                            result_data = {"success": False, "error": "ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨"}

                        # ì„±ê³µ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                        if result_data.get("success"):
                            result_content = f"âœ… **{function_name} ì™„ë£Œ**\n\n"
                            if result_data.get("message"):
                                result_content += f"ğŸ“‹ {result_data['message']}\n\n"

                            # êµ¬ì¡°í™”ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                            if result_data.get("data"):
                                result_content += f"**ê²°ê³¼ ë°ì´í„°:**\n```json\n{json.dumps(result_data['data'], indent=2, ensure_ascii=False)}\n```\n\n"
                        else:
                            result_content = f"âŒ **{function_name} ì‹¤íŒ¨**: {result_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n\n"

                        full_content += result_content

                        # ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                        result_chunk = EnhancedChatStreamChunk(
                            id=ai_message_id,
                            content=result_content,
                            role="assistant",
                            timestamp=datetime.now(),
                            sessionId=session_id,
                            isComplete=False,
                            toolCall=function_name,
                            toolStatus="completed" if result_data.get("success") else "error",
                            toolResult=result_data
                        )
                        yield f"data: {result_chunk.json()}\n\n"

                    except Exception as e:
                        error_content = f"## âŒ {function_name} ì‹¤í–‰ ì˜¤ë¥˜\n\n{str(e)}\n\n"
                        full_content += error_content

                        error_chunk = EnhancedChatStreamChunk(
                            id=ai_message_id,
                            content=error_content,
                            role="assistant",
                            timestamp=datetime.now(),
                            sessionId=session_id,
                            isComplete=False,
                            toolCall=function_name,
                            toolStatus="error"
                        )
                        yield f"data: {error_chunk.json()}\n\n"

                # Tool calls ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
                if any(tool_calls):
                    # Tool calls ë©”ì‹œì§€ ì¶”ê°€
                    messages_with_tools = conversation_messages + [
                        {"role": "assistant", "content": "", "tool_calls": [
                            {
                                "id": tc["id"],
                                "type": "function",
                                "function": {
                                    "name": tc["function"]["name"],
                                    "arguments": tc["function"]["arguments"]
                                }
                            } for tc in tool_calls
                        ]}
                    ]

                    # Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    for tc in tool_calls:
                        tool_result = await tool_manager.registry.execute_tool_call(
                            type('MockToolCall', (), {
                                'function': type('MockFunction', (), {
                                    'name': tc["function"]["name"],
                                    'arguments': tc["function"]["arguments"]
                                })()
                            })()
                        )
                        messages_with_tools.append({
                            "role": "tool",
                            "tool_call_id": tc["id"],
                            "content": tool_result
                        })

                    # ìµœì¢… ì‘ë‹µ ìƒì„±
                    final_response = await client.chat.completions.create(
                        model=request.model,
                        messages=messages_with_tools,
                        stream=True,
                        temperature=0.7
                    )

                    summary_content = "\n\nğŸ’¬ **AI ìš”ì•½:**\n"
                    full_content += summary_content

                    summary_chunk = EnhancedChatStreamChunk(
                        id=ai_message_id,
                        content=summary_content,
                        role="assistant",
                        timestamp=datetime.now(),
                        sessionId=session_id,
                        isComplete=False
                    )
                    yield f"data: {summary_chunk.json()}\n\n"

                    async for chunk in final_response:
                        if chunk.choices[0].delta.content:
                            content_piece = chunk.choices[0].delta.content
                            full_content += content_piece

                            stream_chunk = EnhancedChatStreamChunk(
                                id=ai_message_id,
                                content=content_piece,
                                role="assistant",
                                timestamp=datetime.now(),
                                sessionId=session_id,
                                isComplete=False
                            )
                            yield f"data: {stream_chunk.json()}\n\n"

            # ì™„ë£Œ ì‹ í˜¸
            final_chunk = EnhancedChatStreamChunk(
                id=ai_message_id,
                content="",
                role="assistant",
                timestamp=datetime.now(),
                sessionId=session_id,
                isComplete=True
            )
            yield f"data: {final_chunk.json()}\n\n"

            # AI ì‘ë‹µ ì €ì¥
            ai_message = ChatMessage(
                id=ai_message_id,
                content=full_content,
                role="assistant",
                timestamp=datetime.now(),
                sessionId=session_id
            )
            messages_db[session_id].append(ai_message.dict())
            update_session_message_count(session_id)

        except Exception as e:
            error_msg = f"## âŒ Enhanced Chat ì˜¤ë¥˜\n\n{str(e)}"
            print(error_msg)

            error_chunk = EnhancedChatStreamChunk(
                id=ai_message_id,
                content=error_msg,
                role="assistant",
                timestamp=datetime.now(),
                sessionId=session_id,
                isComplete=True,
                toolStatus="error"
            )
            yield f"data: {error_chunk.json()}\n\n"

    return StreamingResponse(
        generate_enhanced_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )


if __name__ == "__main__":
    import uvicorn

    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
